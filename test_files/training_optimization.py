#!/usr/bin/env python3
"""
è®­ç»ƒå’Œä¼˜åŒ–è„šæœ¬
è§£å†³è¾“å‡ºé”®ç¼ºå¤±é—®é¢˜å¹¶æå‡æ¨¡å‹æ€§èƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import time
from typing import Dict, List, Any, Tuple
from models.advanced_reasoning_net import AdvancedReasoningNet
from evaluators.enhanced_evaluator import EnhancedEvaluator
from evolution.advanced_evolution import AdvancedEvolution
from utils.visualization import EvolutionVisualizer
from config.optimized_logging import setup_optimized_logging

logger = setup_optimized_logging()

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.evaluator = EnhancedEvaluator()
        self.visualizer = EvolutionVisualizer()
        self.training_history = []
        
    async def train_model(self, model: AdvancedReasoningNet, 
                         epochs: int = 50, 
                         learning_rate: float = 0.001) -> AdvancedReasoningNet:
        """è®­ç»ƒæ¨¡å‹"""
        logger.log_important("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ")
        
        model = model.to(self.device)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        train_data = self._generate_training_data(1000)
        
        best_score = 0.0
        best_model = None
        
        for epoch in range(epochs):
            model.train()
            total_loss = 0.0
            batch_count = 0
            
            # åˆ†æ‰¹è®­ç»ƒ
            for i in range(0, len(train_data), 32):
                batch_data = train_data[i:i+32]
                batch_inputs = torch.stack([item[0] for item in batch_data]).to(self.device)
                batch_targets = torch.stack([item[1] for item in batch_data]).to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                outputs = model(batch_inputs)
                
                # è®¡ç®—æŸå¤±
                loss = self._calculate_training_loss(outputs, batch_targets)
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
            
            # è¯„ä¼°æ¨¡å‹
            model.eval()
            with torch.no_grad():
                evaluation_result = await self.evaluator.evaluate_enhanced_reasoning(
                    model=model, max_tasks=20
                )
                current_score = evaluation_result.get('comprehensive_reasoning', 0.0)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(total_loss / batch_count)
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history.append({
                'epoch': epoch,
                'loss': total_loss / batch_count,
                'score': current_score,
                'learning_rate': optimizer.param_groups[0]['lr']
            })
            
            logger.log_important(f"ğŸ”” Epoch {epoch+1}/{epochs}: Loss={total_loss/batch_count:.4f}, Score={current_score:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if current_score > best_score:
                best_score = current_score
                best_model = model.state_dict().copy()
                logger.log_success(f"âœ… æ–°çš„æœ€ä½³åˆ†æ•°: {best_score:.4f}")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model is not None:
            model.load_state_dict(best_model)
            logger.log_success(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³åˆ†æ•°: {best_score:.4f}")
        
        return model
    
    def _generate_training_data(self, num_samples: int) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        data = []
        
        for _ in range(num_samples):
            # ç”Ÿæˆéšæœºè¾“å…¥
            inputs = torch.randn(4)
            
            # ç”Ÿæˆç›®æ ‡è¾“å‡ºï¼ˆåŸºäºè¾“å…¥çš„æ¨¡å¼ï¼‰
            target = torch.zeros(11)  # 11ä¸ªä»»åŠ¡è¾“å‡º
            
            # æ•°å­¦é€»è¾‘ä»»åŠ¡
            if inputs.sum() > 0:
                target[0] = 1.0
            
            # ç¬¦å·æ¨ç†ä»»åŠ¡
            if inputs[0] > inputs[1]:
                target[1] = 1.0
            
            # æŠ½è±¡æ¨ç†ä»»åŠ¡
            if inputs.std() > 0.5:
                target[2] = 1.0
            
            # æ¨¡å¼è¯†åˆ«ä»»åŠ¡
            if torch.all(inputs > 0):
                target[3] = 1.0
            
            # æ¨ç†é“¾ä»»åŠ¡
            if inputs.max() > 1.0:
                target[4] = 1.0
            
            # æ•°å­¦è¯æ˜ä»»åŠ¡
            if inputs.min() < -1.0:
                target[5] = 1.0
            
            # é€»è¾‘é“¾ä»»åŠ¡
            if inputs.mean() > 0:
                target[6] = 1.0
            
            # æŠ½è±¡æ¦‚å¿µä»»åŠ¡
            if inputs.var() > 0.5:
                target[7] = 1.0
            
            # åˆ›é€ æ€§æ¨ç†ä»»åŠ¡
            if torch.abs(inputs).sum() > 2.0:
                target[8] = 1.0
            
            # ç»¼åˆæ¨ç†ä»»åŠ¡
            target[9] = target[:9].mean()
            
            # ç¬¦å·è¡¨è¾¾å¼ä»»åŠ¡
            if inputs[0] * inputs[1] > 0:
                target[10] = 1.0
            
            data.append((inputs, target))
        
        return data
    
    def _calculate_training_loss(self, outputs: Dict[str, torch.Tensor], 
                                targets: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è®­ç»ƒæŸå¤±"""
        loss = 0.0
        
        # å®šä¹‰è¾“å‡ºé”®çš„é¡ºåº
        output_keys = [
            'mathematical_logic', 'symbolic_reasoning', 'abstract_reasoning',
            'pattern_recognition', 'reasoning_chain', 'mathematical_proof',
            'logical_chain', 'abstract_concepts', 'creative_reasoning',
            'comprehensive_reasoning', 'symbolic_expression'
        ]
        
        for i, key in enumerate(output_keys):
            if key in outputs:
                output = outputs[key]
                # ç¡®ä¿è¾“å‡ºåœ¨0-1èŒƒå›´å†…
                output = torch.clamp(output, 0.0, 1.0)
                target = targets[:, i:i+1]
                loss += nn.BCELoss()(output, target)
        
        return loss
    
    async def optimize_model(self, model: AdvancedReasoningNet, 
                           generations: int = 10) -> AdvancedReasoningNet:
        """ä¼˜åŒ–æ¨¡å‹"""
        logger.log_important("ğŸ”„ å¼€å§‹æ¨¡å‹ä¼˜åŒ–")
        
        # åˆ›å»ºè¿›åŒ–ç®—æ³•
        evolution = AdvancedEvolution(
            population_size=8,
            mutation_rate=0.15,
            crossover_rate=0.8,
            elite_size=2
        )
        
        # åˆ›å»ºåˆå§‹ç§ç¾¤
        population = [model]
        for _ in range(7):
            new_model = AdvancedReasoningNet(
                input_size=model.input_size,
                hidden_size=model.hidden_size,
                reasoning_layers=model.reasoning_layers,
                attention_heads=model.attention_heads,
                memory_size=model.memory_size,
                reasoning_types=model.reasoning_types
            )
            new_model.load_state_dict(model.state_dict())
            population.append(new_model)
        
        best_model = model
        best_score = 0.0
        
        for generation in range(generations):
            logger.log_important(f"ğŸ”„ ç¬¬ {generation + 1} ä»£ä¼˜åŒ–")
            
            # è¿›åŒ–
            evolved_population = evolution.evolve(
                population=population,
                evaluator=self.evaluator,
                generations=1
            )
            
            # è¯„ä¼°ç§ç¾¤
            scores = []
            for i, model in enumerate(evolved_population):
                try:
                    evaluation_result = await self.evaluator.evaluate_enhanced_reasoning(
                        model=model, max_tasks=10
                    )
                    score = evaluation_result.get('comprehensive_reasoning', 0.0)
                    scores.append(score)
                    
                    if score > best_score:
                        best_score = score
                        best_model = model
                        logger.log_success(f"âœ… æ–°çš„æœ€ä½³åˆ†æ•°: {best_score:.4f}")
                        
                except Exception as e:
                    logger.log_warning(f"âš ï¸ æ¨¡å‹ {i} è¯„ä¼°å¤±è´¥: {e}")
                    scores.append(0.0)
            
            # è®°å½•è¿›åŒ–å†å²
            avg_score = np.mean(scores)
            logger.log_important(f"ğŸ”” ç¬¬ {generation + 1} ä»£å¹³å‡åˆ†æ•°: {avg_score:.4f}")
            
            # æ›´æ–°ç§ç¾¤
            population = evolved_population
        
        logger.log_success(f"ğŸ‰ ä¼˜åŒ–å®Œæˆï¼æœ€ç»ˆæœ€ä½³åˆ†æ•°: {best_score:.4f}")
        return best_model
    
    def test_model_outputs(self, model: AdvancedReasoningNet) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹è¾“å‡º"""
        logger.log_important("ğŸ” æµ‹è¯•æ¨¡å‹è¾“å‡ºç»“æ„")
        
        model.eval()
        test_input = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)
        
        with torch.no_grad():
            outputs = model(test_input)
        
        # æ£€æŸ¥è¾“å‡ºé”®
        expected_keys = [
            'comprehensive_reasoning', 'symbolic_expression', 'reasoning_chain',
            'mathematical_logic', 'symbolic_reasoning', 'abstract_reasoning',
            'pattern_recognition', 'mathematical_proof', 'logical_chain',
            'abstract_concepts', 'creative_reasoning'
        ]
        
        results = {
            'output_keys': list(outputs.keys()),
            'missing_keys': [],
            'present_keys': [],
            'output_values': {}
        }
        
        for key in expected_keys:
            if key in outputs:
                results['present_keys'].append(key)
                results['output_values'][key] = outputs[key].mean().item()
            else:
                results['missing_keys'].append(key)
        
        logger.log_important(f"ğŸ“Š è¾“å‡ºé”®ç»Ÿè®¡:")
        logger.log_important(f"  æ€»æœŸæœ›é”®æ•°: {len(expected_keys)}")
        logger.log_important(f"  å®é™…è¾“å‡ºé”®æ•°: {len(results['output_keys'])}")
        logger.log_important(f"  ç¼ºå¤±é”®æ•°: {len(results['missing_keys'])}")
        
        if results['missing_keys']:
            logger.log_warning(f"âš ï¸ ç¼ºå¤±çš„é”®: {results['missing_keys']}")
        else:
            logger.log_success("âœ… æ‰€æœ‰æœŸæœ›çš„è¾“å‡ºé”®éƒ½å­˜åœ¨")
        
        return results
    
    def generate_training_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        if not self.training_history:
            return {}
        
        scores = [entry['score'] for entry in self.training_history]
        losses = [entry['loss'] for entry in self.training_history]
        
        report = {
            'total_epochs': len(self.training_history),
            'final_score': scores[-1] if scores else 0.0,
            'best_score': max(scores) if scores else 0.0,
            'final_loss': losses[-1] if losses else 0.0,
            'best_loss': min(losses) if losses else 0.0,
            'score_improvement': scores[-1] - scores[0] if len(scores) > 1 else 0.0,
            'loss_improvement': losses[0] - losses[-1] if len(losses) > 1 else 0.0,
            'training_history': self.training_history
        }
        
        logger.log_important("ğŸ“‹ è®­ç»ƒæŠ¥å‘Š:")
        logger.log_important(f"  æ€»è®­ç»ƒè½®æ•°: {report['total_epochs']}")
        logger.log_important(f"  æœ€ç»ˆåˆ†æ•°: {report['final_score']:.4f}")
        logger.log_important(f"  æœ€ä½³åˆ†æ•°: {report['best_score']:.4f}")
        logger.log_important(f"  åˆ†æ•°æå‡: {report['score_improvement']:.4f}")
        logger.log_important(f"  æœ€ç»ˆæŸå¤±: {report['final_loss']:.4f}")
        logger.log_important(f"  æœ€ä½³æŸå¤±: {report['best_loss']:.4f}")
        logger.log_important(f"  æŸå¤±æ”¹å–„: {report['loss_improvement']:.4f}")
        
        return report

async def main():
    """ä¸»å‡½æ•°"""
    trainer = ModelTrainer()
    
    # åˆ›å»ºæ¨¡å‹
    model = AdvancedReasoningNet(
        input_size=4,
        hidden_size=256,
        reasoning_layers=5,
        attention_heads=8,
        memory_size=20,
        reasoning_types=10
    )
    
    # æµ‹è¯•åˆå§‹è¾“å‡º
    logger.log_important("ğŸ” åˆå§‹æ¨¡å‹è¾“å‡ºæµ‹è¯•")
    initial_test = trainer.test_model_outputs(model)
    
    # è®­ç»ƒæ¨¡å‹
    logger.log_important("ğŸš€ å¼€å§‹è®­ç»ƒ")
    trained_model = await trainer.train_model(model, epochs=30, learning_rate=0.001)
    
    # æµ‹è¯•è®­ç»ƒåè¾“å‡º
    logger.log_important("ğŸ” è®­ç»ƒåæ¨¡å‹è¾“å‡ºæµ‹è¯•")
    trained_test = trainer.test_model_outputs(trained_model)
    
    # ä¼˜åŒ–æ¨¡å‹
    logger.log_important("ğŸ”„ å¼€å§‹ä¼˜åŒ–")
    optimized_model = await trainer.optimize_model(trained_model, generations=5)
    
    # æµ‹è¯•ä¼˜åŒ–åè¾“å‡º
    logger.log_important("ğŸ” ä¼˜åŒ–åæ¨¡å‹è¾“å‡ºæµ‹è¯•")
    optimized_test = trainer.test_model_outputs(optimized_model)
    
    # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
    training_report = trainer.generate_training_report()
    
    # æœ€ç»ˆè¯„ä¼°
    logger.log_important("ğŸ“Š æœ€ç»ˆè¯„ä¼°")
    final_evaluation = await trainer.evaluator.evaluate_enhanced_reasoning(
        model=optimized_model, max_tasks=20
    )
    
    logger.log_important("ğŸ‰ è®­ç»ƒå’Œä¼˜åŒ–å®Œæˆï¼")
    logger.log_important(f"ğŸ“Š æœ€ç»ˆç»¼åˆæ¨ç†åˆ†æ•°: {final_evaluation.get('comprehensive_reasoning', 0.0):.4f}")

if __name__ == "__main__":
    asyncio.run(main()) 