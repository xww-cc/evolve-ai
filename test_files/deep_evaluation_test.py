#!/usr/bin/env python3
"""
æ·±åº¦è¯„ä¼°æµ‹è¯•
è¿›è¡Œæ›´å…¨é¢çš„ç³»ç»Ÿæµ‹è¯•å’Œè¯„ä¼°
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio
import numpy as np
import torch
import time
import json
from typing import Dict, List, Any, Tuple
from models.advanced_reasoning_net import AdvancedReasoningNet
from evaluators.enhanced_evaluator import EnhancedEvaluator
from evolution.advanced_evolution import AdvancedEvolution, MultiObjectiveAdvancedEvolution
from utils.visualization import EvolutionVisualizer
from config.optimized_logging import setup_optimized_logging

logger = setup_optimized_logging()

class DeepEvaluator:
    """æ·±åº¦è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.evaluator = EnhancedEvaluator()
        self.visualizer = EvolutionVisualizer()
        self.test_results = {}
        
    async def test_model_scalability(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹å¯æ‰©å±•æ€§"""
        logger.log_important("ğŸ” æµ‹è¯•æ¨¡å‹å¯æ‰©å±•æ€§")
        
        scalability_results = {
            'small_models': [],
            'medium_models': [],
            'large_models': [],
            'performance_metrics': {}
        }
        
        # æµ‹è¯•ä¸åŒè§„æ¨¡çš„æ¨¡å‹
        model_configs = [
            # å°æ¨¡å‹
            {'hidden_size': 64, 'reasoning_layers': 2, 'attention_heads': 4, 'memory_size': 10, 'reasoning_types': 5},
            {'hidden_size': 128, 'reasoning_layers': 3, 'attention_heads': 4, 'memory_size': 15, 'reasoning_types': 8},
            # ä¸­ç­‰æ¨¡å‹
            {'hidden_size': 256, 'reasoning_layers': 4, 'attention_heads': 8, 'memory_size': 20, 'reasoning_types': 10},
            {'hidden_size': 384, 'reasoning_layers': 5, 'attention_heads': 12, 'memory_size': 25, 'reasoning_types': 12},
            # å¤§æ¨¡å‹
            {'hidden_size': 512, 'reasoning_layers': 6, 'attention_heads': 16, 'memory_size': 30, 'reasoning_types': 15},
            {'hidden_size': 768, 'reasoning_layers': 8, 'attention_heads': 24, 'memory_size': 40, 'reasoning_types': 20}
        ]
        
        for i, config in enumerate(model_configs):
            try:
                # ç¡®ä¿hidden_sizeèƒ½è¢«attention_headsæ•´é™¤
                adjusted_hidden = (config['hidden_size'] // config['attention_heads']) * config['attention_heads']
                
                model = AdvancedReasoningNet(
                    input_size=4,
                    hidden_size=adjusted_hidden,
                    reasoning_layers=config['reasoning_layers'],
                    attention_heads=config['attention_heads'],
                    memory_size=config['memory_size'],
                    reasoning_types=config['reasoning_types']
                )
                
                # æµ‹è¯•æ¨ç†æ€§èƒ½
                start_time = time.time()
                test_input = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)
                with torch.no_grad():
                    output = model(test_input)
                inference_time = (time.time() - start_time) * 1000  # æ¯«ç§’
                
                # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
                total_params = sum(p.numel() for p in model.parameters())
                
                # è¯„ä¼°æ¨ç†èƒ½åŠ›
                reasoning_score = await self.evaluator.evaluate_enhanced_reasoning(model, max_tasks=5)
                
                result = {
                    'config': config,
                    'adjusted_hidden_size': adjusted_hidden,
                    'total_params': total_params,
                    'inference_time_ms': inference_time,
                    'reasoning_score': reasoning_score.get('comprehensive_reasoning', 0.0),
                    'success': True
                }
                
                if i < 2:
                    scalability_results['small_models'].append(result)
                elif i < 4:
                    scalability_results['medium_models'].append(result)
                else:
                    scalability_results['large_models'].append(result)
                    
                logger.log_success(f"âœ… æ¨¡å‹ {i+1} æµ‹è¯•æˆåŠŸ: {total_params:,} å‚æ•°, {inference_time:.2f}ms")
                
            except Exception as e:
                logger.log_error(f"âŒ æ¨¡å‹ {i+1} æµ‹è¯•å¤±è´¥: {e}")
                result = {'config': config, 'success': False, 'error': str(e)}
                if i < 2:
                    scalability_results['small_models'].append(result)
                elif i < 4:
                    scalability_results['medium_models'].append(result)
                else:
                    scalability_results['large_models'].append(result)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        all_successful = [r for r in scalability_results['small_models'] + 
                         scalability_results['medium_models'] + 
                         scalability_results['large_models'] if r.get('success', False)]
        
        if all_successful:
            scalability_results['performance_metrics'] = {
                'avg_inference_time': np.mean([r['inference_time_ms'] for r in all_successful]),
                'avg_reasoning_score': np.mean([r['reasoning_score'] for r in all_successful]),
                'total_params_range': (min([r['total_params'] for r in all_successful]), 
                                     max([r['total_params'] for r in all_successful]))
            }
        
        return scalability_results
    
    def test_evolution_convergence(self) -> Dict[str, Any]:
        """æµ‹è¯•è¿›åŒ–æ”¶æ•›æ€§"""
        logger.log_important("ğŸ” æµ‹è¯•è¿›åŒ–æ”¶æ•›æ€§")
        
        # åˆ›å»ºåˆå§‹ç§ç¾¤
        population = []
        for i in range(8):
            model = AdvancedReasoningNet(
                input_size=4,
                hidden_size=256,
                reasoning_layers=5,
                attention_heads=8,
                memory_size=20,
                reasoning_types=10
            )
            population.append(model)
        
        # åˆ›å»ºè¿›åŒ–ç®—æ³•
        evolution = AdvancedEvolution(
            population_size=8,
            mutation_rate=0.15,
            crossover_rate=0.8,
            elite_size=2
        )
        
        convergence_data = {
            'generations': [],
            'best_fitness': [],
            'avg_fitness': [],
            'diversity_scores': [],
            'convergence_metrics': {}
        }
        
        # è¿è¡Œå¤šä»£è¿›åŒ–
        for gen in range(10):
            logger.log_important(f"ğŸ”„ ç¬¬ {gen+1} ä»£è¿›åŒ–")
            
            # è®¡ç®—é€‚åº”åº¦
            fitness_scores = []
            for model in population:
                score = evolution._calculate_fitness(model, self.evaluator)
                fitness_scores.append(score)
            
            # è®¡ç®—å¤šæ ·æ€§
            diversity = evolution._calculate_diversity(population)
            
            # è®°å½•æ•°æ®
            convergence_data['generations'].append(gen + 1)
            convergence_data['best_fitness'].append(max(fitness_scores))
            convergence_data['avg_fitness'].append(np.mean(fitness_scores))
            convergence_data['diversity_scores'].append(diversity)
            
            logger.log_important(f"  æœ€ä½³é€‚åº”åº¦: {max(fitness_scores):.4f}")
            logger.log_important(f"  å¹³å‡é€‚åº”åº¦: {np.mean(fitness_scores):.4f}")
            logger.log_important(f"  å¤šæ ·æ€§: {diversity:.4f}")
            
            # è¿›åŒ–åˆ°ä¸‹ä¸€ä»£
            population = evolution.evolve(population, self.evaluator, generations=1)
        
        # è®¡ç®—æ”¶æ•›æŒ‡æ ‡
        best_fitness = convergence_data['best_fitness']
        convergence_data['convergence_metrics'] = {
            'final_best_fitness': best_fitness[-1],
            'fitness_improvement': best_fitness[-1] - best_fitness[0],
            'convergence_rate': (best_fitness[-1] - best_fitness[0]) / len(best_fitness),
            'stability': np.std(best_fitness[-3:]) if len(best_fitness) >= 3 else 0
        }
        
        return convergence_data
    
    async def test_multi_objective_optimization(self) -> Dict[str, Any]:
        """æµ‹è¯•å¤šç›®æ ‡ä¼˜åŒ–"""
        logger.log_important("ğŸ” æµ‹è¯•å¤šç›®æ ‡ä¼˜åŒ–")
        
        # åˆ›å»ºç§ç¾¤
        population = []
        for i in range(10):
            model = AdvancedReasoningNet(
                input_size=4,
                hidden_size=256,
                reasoning_layers=5,
                attention_heads=8,
                memory_size=20,
                reasoning_types=10
            )
            population.append(model)
        
        # åˆ›å»ºå¤šç›®æ ‡è¿›åŒ–ç®—æ³•
        multi_evolution = MultiObjectiveAdvancedEvolution(population_size=10)
        
        # è®¡ç®—å¤šç›®æ ‡
        objectives = {
            'reasoning_ability': [],
            'efficiency': [],
            'complexity': []
        }
        
        for model in population:
            # æ¨ç†èƒ½åŠ›
            reasoning_score = await self.evaluator.evaluate_enhanced_reasoning(model, max_tasks=5)
            objectives['reasoning_ability'].append(reasoning_score.get('comprehensive_reasoning', 0.0))
            
            # æ•ˆç‡ï¼ˆæ¨ç†æ—¶é—´ï¼‰
            start_time = time.time()
            test_input = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)
            with torch.no_grad():
                model(test_input)
            inference_time = time.time() - start_time
            objectives['efficiency'].append(1.0 / (1.0 + inference_time * 1000))  # è½¬æ¢ä¸º0-1èŒƒå›´
            
            # å¤æ‚åº¦ï¼ˆå‚æ•°æ•°é‡ï¼‰
            total_params = sum(p.numel() for p in model.parameters())
            objectives['complexity'].append(1.0 / (1.0 + total_params / 1000000))  # è½¬æ¢ä¸º0-1èŒƒå›´
        
        # è¿è¡Œå¤šç›®æ ‡è¿›åŒ–
        evolved_population = await multi_evolution.evolve_multi_objective(population, objectives)
        
        # è¯„ä¼°æœ€ç»ˆç»“æœ
        final_objectives = {
            'reasoning_ability': [],
            'efficiency': [],
            'complexity': []
        }
        
        for model in evolved_population:
            reasoning_score = await self.evaluator.evaluate_enhanced_reasoning(model, max_tasks=5)
            final_objectives['reasoning_ability'].append(reasoning_score.get('comprehensive_reasoning', 0.0))
            
            start_time = time.time()
            test_input = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)
            with torch.no_grad():
                model(test_input)
            inference_time = time.time() - start_time
            final_objectives['efficiency'].append(1.0 / (1.0 + inference_time * 1000))
            
            total_params = sum(p.numel() for p in model.parameters())
            final_objectives['complexity'].append(1.0 / (1.0 + total_params / 1000000))
        
        return {
            'initial_objectives': objectives,
            'final_objectives': final_objectives,
            'population_size': len(evolved_population),
            'improvement_metrics': {
                'reasoning_improvement': np.mean(final_objectives['reasoning_ability']) - np.mean(objectives['reasoning_ability']),
                'efficiency_improvement': np.mean(final_objectives['efficiency']) - np.mean(objectives['efficiency']),
                'complexity_improvement': np.mean(final_objectives['complexity']) - np.mean(objectives['complexity'])
            }
        }
    
    def test_robustness_and_stability(self) -> Dict[str, Any]:
        """æµ‹è¯•é²æ£’æ€§å’Œç¨³å®šæ€§"""
        logger.log_important("ğŸ” æµ‹è¯•é²æ£’æ€§å’Œç¨³å®šæ€§")
        
        robustness_results = {
            'error_handling': [],
            'memory_usage': [],
            'performance_consistency': [],
            'stress_tests': []
        }
        
        # 1. é”™è¯¯å¤„ç†æµ‹è¯•
        logger.log_important("ğŸ” é”™è¯¯å¤„ç†æµ‹è¯•")
        try:
            # æµ‹è¯•æ— æ•ˆè¾“å…¥
            model = AdvancedReasoningNet()
            invalid_input = torch.tensor([[1, 2, 3]], dtype=torch.float32)  # ç»´åº¦ä¸åŒ¹é…
            with torch.no_grad():
                output = model(invalid_input)
            robustness_results['error_handling'].append({
                'test': 'invalid_input_dimension',
                'success': True,
                'message': 'æ­£ç¡®å¤„ç†äº†ç»´åº¦ä¸åŒ¹é…'
            })
        except Exception as e:
            robustness_results['error_handling'].append({
                'test': 'invalid_input_dimension',
                'success': False,
                'error': str(e)
            })
        
        # 2. å†…å­˜ä½¿ç”¨æµ‹è¯•
        logger.log_important("ğŸ” å†…å­˜ä½¿ç”¨æµ‹è¯•")
        import psutil
        process = psutil.Process()
        
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        models = []
        for i in range(5):
            model = AdvancedReasoningNet()
            models.append(model)
            
            current_memory = process.memory_info().rss / 1024 / 1024
            memory_increase = current_memory - initial_memory
            
            robustness_results['memory_usage'].append({
                'model_count': i + 1,
                'memory_mb': current_memory,
                'memory_increase_mb': memory_increase
            })
        
        # 3. æ€§èƒ½ä¸€è‡´æ€§æµ‹è¯•
        logger.log_important("ğŸ” æ€§èƒ½ä¸€è‡´æ€§æµ‹è¯•")
        model = AdvancedReasoningNet()
        inference_times = []
        
        for i in range(10):
            start_time = time.time()
            test_input = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)
            with torch.no_grad():
                model(test_input)
            inference_time = (time.time() - start_time) * 1000
            inference_times.append(inference_time)
        
        robustness_results['performance_consistency'] = {
            'mean_inference_time': np.mean(inference_times),
            'std_inference_time': np.std(inference_times),
            'cv_inference_time': np.std(inference_times) / np.mean(inference_times),  # å˜å¼‚ç³»æ•°
            'all_times': inference_times
        }
        
        # 4. å‹åŠ›æµ‹è¯•
        logger.log_important("ğŸ” å‹åŠ›æµ‹è¯•")
        try:
            # è¿ç»­æ¨ç†æµ‹è¯•
            model = AdvancedReasoningNet()
            start_time = time.time()
            
            for i in range(100):
                test_input = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)
                with torch.no_grad():
                    model(test_input)
            
            total_time = time.time() - start_time
            avg_time = total_time / 100 * 1000  # æ¯«ç§’
            
            robustness_results['stress_tests'].append({
                'test': 'continuous_inference',
                'iterations': 100,
                'total_time_seconds': total_time,
                'avg_time_ms': avg_time,
                'success': True
            })
            
        except Exception as e:
            robustness_results['stress_tests'].append({
                'test': 'continuous_inference',
                'success': False,
                'error': str(e)
            })
        
        return robustness_results
    
    async def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        logger.log_important("ğŸ“‹ ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š")
        
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'test_summary': {},
            'detailed_results': {},
            'recommendations': []
        }
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        logger.log_important("ğŸ” è¿è¡Œå¯æ‰©å±•æ€§æµ‹è¯•")
        scalability_results = await self.test_model_scalability()
        report['detailed_results']['scalability'] = scalability_results
        
        logger.log_important("ğŸ” è¿è¡Œæ”¶æ•›æ€§æµ‹è¯•")
        convergence_results = self.test_evolution_convergence()
        report['detailed_results']['convergence'] = convergence_results
        
        logger.log_important("ğŸ” è¿è¡Œå¤šç›®æ ‡ä¼˜åŒ–æµ‹è¯•")
        multi_objective_results = await self.test_multi_objective_optimization()
        report['detailed_results']['multi_objective'] = multi_objective_results
        
        logger.log_important("ğŸ” è¿è¡Œé²æ£’æ€§æµ‹è¯•")
        robustness_results = self.test_robustness_and_stability()
        report['detailed_results']['robustness'] = robustness_results
        
        # ç”Ÿæˆæµ‹è¯•æ€»ç»“
        total_tests = 0
        passed_tests = 0
        
        # å¯æ‰©å±•æ€§æµ‹è¯•ç»Ÿè®¡
        scalability_success = sum(1 for r in scalability_results['small_models'] + 
                                scalability_results['medium_models'] + 
                                scalability_results['large_models'] if r.get('success', False))
        total_scalability = len(scalability_results['small_models'] + 
                              scalability_results['medium_models'] + 
                              scalability_results['large_models'])
        total_tests += total_scalability
        passed_tests += scalability_success
        
        # æ”¶æ•›æ€§æµ‹è¯•ç»Ÿè®¡
        if convergence_results['convergence_metrics']['final_best_fitness'] > 0:
            passed_tests += 1
        total_tests += 1
        
        # å¤šç›®æ ‡ä¼˜åŒ–æµ‹è¯•ç»Ÿè®¡
        if multi_objective_results['improvement_metrics']['reasoning_improvement'] > 0:
            passed_tests += 1
        total_tests += 1
        
        # é²æ£’æ€§æµ‹è¯•ç»Ÿè®¡
        robustness_success = sum(1 for r in robustness_results['error_handling'] if r.get('success', False))
        robustness_success += sum(1 for r in robustness_results['stress_tests'] if r.get('success', False))
        total_robustness = len(robustness_results['error_handling']) + len(robustness_results['stress_tests'])
        total_tests += total_robustness
        passed_tests += robustness_success
        
        report['test_summary'] = {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,
            'scalability_success_rate': (scalability_success / total_scalability * 100) if total_scalability > 0 else 0,
            'robustness_success_rate': (robustness_success / total_robustness * 100) if total_robustness > 0 else 0
        }
        
        # ç”Ÿæˆå»ºè®®
        if report['test_summary']['success_rate'] < 90:
            report['recommendations'].append("éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ç³»ç»Ÿç¨³å®šæ€§")
        
        if scalability_results['performance_metrics'].get('avg_inference_time', 0) > 10:
            report['recommendations'].append("æ¨ç†æ€§èƒ½éœ€è¦ä¼˜åŒ–")
        
        if convergence_results['convergence_metrics']['stability'] > 0.1:
            report['recommendations'].append("è¿›åŒ–æ”¶æ•›æ€§éœ€è¦æ”¹è¿›")
        
        return report

async def main():
    """ä¸»å‡½æ•°"""
    evaluator = DeepEvaluator()
    
    logger.log_important("ğŸš€ å¼€å§‹æ·±åº¦è¯„ä¼°æµ‹è¯•")
    logger.log_important("=" * 60)
    
    # è¿è¡Œæ·±åº¦è¯„ä¼°
    report = await evaluator.generate_comprehensive_report()
    
    # è¾“å‡ºç»“æœ
    logger.log_important("ğŸ“‹ æ·±åº¦è¯„ä¼°æŠ¥å‘Š")
    logger.log_important("=" * 60)
    
    summary = report['test_summary']
    logger.log_important(f"ğŸ“Š æµ‹è¯•æ€»ç»“:")
    logger.log_important(f"  æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
    logger.log_important(f"  é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
    logger.log_important(f"  æˆåŠŸç‡: {summary['success_rate']:.1f}%")
    logger.log_important(f"  å¯æ‰©å±•æ€§æˆåŠŸç‡: {summary['scalability_success_rate']:.1f}%")
    logger.log_important(f"  é²æ£’æ€§æˆåŠŸç‡: {summary['robustness_success_rate']:.1f}%")
    
    if report['recommendations']:
        logger.log_important(f"ğŸ’¡ å»ºè®®:")
        for rec in report['recommendations']:
            logger.log_important(f"  - {rec}")
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"evaluation_report_{int(time.time())}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.log_important(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    if summary['success_rate'] >= 90:
        logger.log_success("ğŸ‰ æ·±åº¦è¯„ä¼°æµ‹è¯•æˆåŠŸï¼ç³»ç»Ÿè¡¨ç°ä¼˜ç§€")
    elif summary['success_rate'] >= 70:
        logger.log_success("âœ… æ·±åº¦è¯„ä¼°æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¡¨ç°è‰¯å¥½")
    else:
        logger.log_warning("âš ï¸ æ·±åº¦è¯„ä¼°æµ‹è¯•éƒ¨åˆ†é€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

if __name__ == "__main__":
    asyncio.run(main()) 