#!/usr/bin/env python3
"""
å…¨é¢ç³»ç»Ÿå¤æµ‹è„šæœ¬
éªŒè¯æ‰€æœ‰ä¼˜åŒ–æ•ˆæœå’Œç³»ç»Ÿç¨³å®šæ€§
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio
import numpy as np
import torch
import time
from models.advanced_reasoning_net import AdvancedReasoningNet
from evaluators.enhanced_evaluator import EnhancedEvaluator
from evolution.advanced_evolution import AdvancedEvolution
from config.optimized_logging import setup_optimized_logging
from utils.visualization import EvolutionVisualizer
import matplotlib.pyplot as plt

logger = setup_optimized_logging()

class ComprehensiveRetest:
    """å…¨é¢ç³»ç»Ÿå¤æµ‹å™¨"""
    
    def __init__(self):
        self.test_results = {}
        self.performance_metrics = {}
        self.optimization_verification = {}
        
    async def run_comprehensive_retest(self):
        """è¿è¡Œå…¨é¢å¤æµ‹"""
        logger.log_important("ğŸ”„ å¼€å§‹å…¨é¢ç³»ç»Ÿå¤æµ‹")
        logger.log_important("=" * 60)
        
        # 1. æ¨ç†èƒ½åŠ›å¤æµ‹
        await self._retest_reasoning_capabilities()
        
        # 2. ç³»ç»Ÿç¨³å®šæ€§å¤æµ‹
        await self._retest_system_stability()
        
        # 3. æ€§èƒ½æŒ‡æ ‡å¤æµ‹
        await self._retest_performance_metrics()
        
        # 4. è¿›åŒ–ç®—æ³•å¤æµ‹
        await self._retest_evolution_algorithm()
        
        # 5. è¯„ä¼°å™¨å¤æµ‹
        await self._retest_evaluators()
        
        # 6. å¯è§†åŒ–åŠŸèƒ½å¤æµ‹
        await self._retest_visualization()
        
        # 7. ä¼˜åŒ–æ•ˆæœéªŒè¯
        await self._verify_optimization_effects()
        
        # 8. ç”Ÿæˆå¤æµ‹æŠ¥å‘Š
        self._generate_retest_report()
        
        return self.test_results
    
    async def _retest_reasoning_capabilities(self):
        """å¤æµ‹æ¨ç†èƒ½åŠ›"""
        logger.log_important("ğŸ§  1. æ¨ç†èƒ½åŠ›å¤æµ‹")
        logger.log_important("-" * 40)
        
        # ä½¿ç”¨æœ€ä½³é…ç½®
        best_config = {
            'hidden_size': 4096,
            'reasoning_layers': 8,
            'attention_heads': 128,
            'memory_size': 300,
            'reasoning_types': 25
        }
        
        model = AdvancedReasoningNet(
            input_size=4,
            hidden_size=best_config['hidden_size'],
            reasoning_layers=best_config['reasoning_layers'],
            attention_heads=best_config['attention_heads'],
            memory_size=best_config['memory_size'],
            reasoning_types=best_config['reasoning_types']
        )
        
        evaluator = EnhancedEvaluator()
        
        # å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
        reasoning_scores = []
        inference_times = []
        
        for i in range(3):
            start_time = time.time()
            result = await evaluator.evaluate_enhanced_reasoning(model, max_tasks=8)
            end_time = time.time()
            
            reasoning_score = result.get('comprehensive_reasoning', 0.0)
            inference_time = (end_time - start_time) * 1000
            
            reasoning_scores.append(reasoning_score)
            inference_times.append(inference_time)
            
            logger.log_important(f"   æµ‹è¯• {i+1}: æ¨ç†åˆ†æ•°={reasoning_score:.4f}, æ—¶é—´={inference_time:.2f}ms")
        
        avg_reasoning_score = np.mean(reasoning_scores)
        avg_inference_time = np.mean(inference_times)
        score_std = np.std(reasoning_scores)
        
        self.test_results['reasoning_capabilities'] = {
            'avg_score': avg_reasoning_score,
            'avg_time': avg_inference_time,
            'score_std': score_std,
            'target_achieved': avg_reasoning_score >= 0.1,
            'scores': reasoning_scores,
            'times': inference_times
        }
        
        logger.log_important(f"ğŸ“Š æ¨ç†èƒ½åŠ›å¤æµ‹ç»“æœ:")
        logger.log_important(f"   å¹³å‡æ¨ç†åˆ†æ•°: {avg_reasoning_score:.4f}")
        logger.log_important(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.2f}ms")
        logger.log_important(f"   åˆ†æ•°æ ‡å‡†å·®: {score_std:.4f}")
        
        if avg_reasoning_score >= 0.1:
            logger.log_success("âœ… æ¨ç†èƒ½åŠ›å¤æµ‹é€šè¿‡ï¼Œç›®æ ‡è¾¾æˆ")
        else:
            logger.log_warning(f"âš ï¸ æ¨ç†èƒ½åŠ›å¤æµ‹æœªè¾¾æ ‡ï¼Œéœ€è¦æ”¹è¿›")
    
    async def _retest_system_stability(self):
        """å¤æµ‹ç³»ç»Ÿç¨³å®šæ€§"""
        logger.log_important("\nğŸ”§ 2. ç³»ç»Ÿç¨³å®šæ€§å¤æµ‹")
        logger.log_important("-" * 40)
        
        stability_tests = []
        
        # æµ‹è¯•1: æ¨¡å‹åˆ›å»ºç¨³å®šæ€§
        try:
            models = []
            for i in range(5):
                model = AdvancedReasoningNet(
                    input_size=4,
                    hidden_size=256 + i * 100,
                    reasoning_layers=5 + i,
                    attention_heads=8 + i * 2,
                    memory_size=20 + i * 10,
                    reasoning_types=10 + i
                )
                models.append(model)
            stability_tests.append(('æ¨¡å‹åˆ›å»º', True, 'æˆåŠŸåˆ›å»º5ä¸ªä¸åŒé…ç½®çš„æ¨¡å‹'))
        except Exception as e:
            stability_tests.append(('æ¨¡å‹åˆ›å»º', False, f'å¤±è´¥: {e}'))
        
        # æµ‹è¯•2: æ¨ç†ç¨³å®šæ€§
        try:
            model = AdvancedReasoningNet(input_size=4, hidden_size=256, reasoning_layers=5)
            test_input = torch.randn(10, 4)
            
            outputs = []
            for _ in range(10):
                with torch.no_grad():
                    output = model(test_input)
                    outputs.append(output)
            
            # æ£€æŸ¥è¾“å‡ºä¸€è‡´æ€§
            if isinstance(outputs[0], dict):
                output_keys = outputs[0].keys()
                consistency_check = all(
                    all(key in output.keys() for key in output_keys) 
                    for output in outputs
                )
            else:
                consistency_check = all(
                    output.shape == outputs[0].shape 
                    for output in outputs
                )
            
            stability_tests.append(('æ¨ç†ç¨³å®šæ€§', consistency_check, 'è¿ç»­æ¨ç†è¾“å‡ºä¸€è‡´'))
        except Exception as e:
            stability_tests.append(('æ¨ç†ç¨³å®šæ€§', False, f'å¤±è´¥: {e}'))
        
        # æµ‹è¯•3: å†…å­˜ç¨³å®šæ€§
        try:
            import psutil
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # åˆ›å»ºå¤šä¸ªæ¨¡å‹æµ‹è¯•å†…å­˜
            models = []
            for i in range(3):
                model = AdvancedReasoningNet(
                    input_size=4,
                    hidden_size=512,
                    reasoning_layers=6,
                    attention_heads=12,
                    memory_size=30,
                    reasoning_types=15
                )
                models.append(model)
            
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - initial_memory
            
            # æ¸…ç†å†…å­˜
            del models
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            stability_tests.append(('å†…å­˜ç¨³å®šæ€§', memory_increase < 1000, f'å†…å­˜å¢åŠ : {memory_increase:.1f}MB'))
        except Exception as e:
            stability_tests.append(('å†…å­˜ç¨³å®šæ€§', False, f'å¤±è´¥: {e}'))
        
        # ç»Ÿè®¡ç»“æœ
        passed_tests = sum(1 for test in stability_tests if test[1])
        total_tests = len(stability_tests)
        stability_rate = passed_tests / total_tests * 100
        
        self.test_results['system_stability'] = {
            'stability_rate': stability_rate,
            'passed_tests': passed_tests,
            'total_tests': total_tests,
            'test_details': stability_tests
        }
        
        logger.log_important(f"ğŸ“Š ç³»ç»Ÿç¨³å®šæ€§å¤æµ‹ç»“æœ:")
        for test_name, passed, description in stability_tests:
            status = "âœ…" if passed else "âŒ"
            logger.log_important(f"   {status} {test_name}: {description}")
        
        logger.log_important(f"   ç¨³å®šæ€§é€šè¿‡ç‡: {stability_rate:.1f}% ({passed_tests}/{total_tests})")
        
        if stability_rate >= 90:
            logger.log_success("âœ… ç³»ç»Ÿç¨³å®šæ€§å¤æµ‹é€šè¿‡")
        else:
            logger.log_warning(f"âš ï¸ ç³»ç»Ÿç¨³å®šæ€§éœ€è¦æ”¹è¿›")
    
    async def _retest_performance_metrics(self):
        """å¤æµ‹æ€§èƒ½æŒ‡æ ‡"""
        logger.log_important("\nâš¡ 3. æ€§èƒ½æŒ‡æ ‡å¤æµ‹")
        logger.log_important("-" * 40)
        
        # æµ‹è¯•ä¸åŒè§„æ¨¡çš„æ¨¡å‹æ€§èƒ½
        performance_configs = [
            {'name': 'å°å‹æ¨¡å‹', 'hidden_size': 128, 'reasoning_layers': 3},
            {'name': 'ä¸­å‹æ¨¡å‹', 'hidden_size': 512, 'reasoning_layers': 6},
            {'name': 'å¤§å‹æ¨¡å‹', 'hidden_size': 1024, 'reasoning_layers': 8},
            {'name': 'è¶…å¤§å‹æ¨¡å‹', 'hidden_size': 2048, 'reasoning_layers': 10}
        ]
        
        performance_results = []
        
        for config in performance_configs:
            try:
                model = AdvancedReasoningNet(
                    input_size=4,
                    hidden_size=config['hidden_size'],
                    reasoning_layers=config['reasoning_layers'],
                    attention_heads=8,
                    memory_size=20,
                    reasoning_types=10
                )
                
                # æµ‹è¯•æ¨ç†æ—¶é—´
                test_input = torch.randn(1, 4)
                
                # é¢„çƒ­
                with torch.no_grad():
                    for _ in range(3):
                        _ = model(test_input)
                
                # æ­£å¼æµ‹è¯•
                times = []
                for _ in range(10):
                    start_time = time.time()
                    with torch.no_grad():
                        _ = model(test_input)
                    end_time = time.time()
                    times.append((end_time - start_time) * 1000)
                
                avg_time = np.mean(times)
                std_time = np.std(times)
                
                # æµ‹è¯•å†…å­˜ä½¿ç”¨
                import psutil
                process = psutil.Process()
                memory_usage = process.memory_info().rss / 1024 / 1024  # MB
                
                performance_results.append({
                    'name': config['name'],
                    'avg_time': avg_time,
                    'std_time': std_time,
                    'memory_usage': memory_usage,
                    'success': True
                })
                
                logger.log_important(f"   {config['name']}: {avg_time:.2f}ms Â± {std_time:.2f}ms, {memory_usage:.1f}MB")
                
            except Exception as e:
                performance_results.append({
                    'name': config['name'],
                    'avg_time': 0,
                    'std_time': 0,
                    'memory_usage': 0,
                    'success': False,
                    'error': str(e)
                })
                logger.log_error(f"   {config['name']}: æµ‹è¯•å¤±è´¥ - {e}")
        
        self.test_results['performance_metrics'] = {
            'configs': performance_results,
            'success_rate': sum(1 for r in performance_results if r['success']) / len(performance_results) * 100
        }
        
        logger.log_important(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡å¤æµ‹å®Œæˆï¼ŒæˆåŠŸç‡: {self.test_results['performance_metrics']['success_rate']:.1f}%")
    
    async def _retest_evolution_algorithm(self):
        """å¤æµ‹è¿›åŒ–ç®—æ³•"""
        logger.log_important("\nğŸ”„ 4. è¿›åŒ–ç®—æ³•å¤æµ‹")
        logger.log_important("-" * 40)
        
        try:
            # åˆ›å»ºè¿›åŒ–ç®—æ³•
            evolution = AdvancedEvolution(
                population_size=6,
                mutation_rate=0.1,
                crossover_rate=0.8,
                elite_size=1
            )
            
            # åˆ›å»ºåˆå§‹ç§ç¾¤
            population = []
            for i in range(6):
                model = AdvancedReasoningNet(
                    input_size=4,
                    hidden_size=256 + i * 50,
                    reasoning_layers=5 + i,
                    attention_heads=8 + i,
                    memory_size=20,
                    reasoning_types=10
                )
                population.append(model)
            
            # è¿è¡Œè¿›åŒ–
            start_time = time.time()
            evolved_population, history = await evolution.evolve_population(
                population, 
                generations=3,
                evaluator=EnhancedEvaluator()
            )
            end_time = time.time()
            
            evolution_time = end_time - start_time
            
            # åˆ†æç»“æœ
            if history and len(history) > 0:
                best_fitness = max(history[-1]['best_fitness'] for history in history)
                avg_fitness = np.mean([h['avg_fitness'] for h in history])
                diversity = np.mean([h.get('diversity', 0) for h in history])
            else:
                best_fitness = 0
                avg_fitness = 0
                diversity = 0
            
            self.test_results['evolution_algorithm'] = {
                'evolution_time': evolution_time,
                'best_fitness': best_fitness,
                'avg_fitness': avg_fitness,
                'diversity': diversity,
                'population_size': len(evolved_population),
                'success': True
            }
            
            logger.log_important(f"ğŸ“Š è¿›åŒ–ç®—æ³•å¤æµ‹ç»“æœ:")
            logger.log_important(f"   è¿›åŒ–æ—¶é—´: {evolution_time:.2f}ç§’")
            logger.log_important(f"   æœ€ä½³é€‚åº”åº¦: {best_fitness:.4f}")
            logger.log_important(f"   å¹³å‡é€‚åº”åº¦: {avg_fitness:.4f}")
            logger.log_important(f"   å¤šæ ·æ€§: {diversity:.4f}")
            logger.log_important(f"   ç§ç¾¤å¤§å°: {len(evolved_population)}")
            
            logger.log_success("âœ… è¿›åŒ–ç®—æ³•å¤æµ‹é€šè¿‡")
            
        except Exception as e:
            logger.log_error(f"âŒ è¿›åŒ–ç®—æ³•å¤æµ‹å¤±è´¥: {e}")
            self.test_results['evolution_algorithm'] = {
                'success': False,
                'error': str(e)
            }
    
    async def _retest_evaluators(self):
        """å¤æµ‹è¯„ä¼°å™¨"""
        logger.log_important("\nğŸ“Š 5. è¯„ä¼°å™¨å¤æµ‹")
        logger.log_important("-" * 40)
        
        try:
            evaluator = EnhancedEvaluator()
            model = AdvancedReasoningNet(
                input_size=4,
                hidden_size=256,
                reasoning_layers=5,
                attention_heads=8,
                memory_size=20,
                reasoning_types=10
            )
            
            # æµ‹è¯•è¯„ä¼°å™¨
            start_time = time.time()
            result = await evaluator.evaluate_enhanced_reasoning(model, max_tasks=5)
            end_time = time.time()
            
            evaluation_time = end_time - start_time
            
            # æ£€æŸ¥è¯„ä¼°ç»“æœå®Œæ•´æ€§
            expected_keys = [
                'comprehensive_reasoning', 'nested_reasoning', 'symbolic_induction',
                'graph_reasoning', 'multi_step_chain', 'logical_chain',
                'abstract_concept', 'creative_reasoning', 'symbolic_expression'
            ]
            
            missing_keys = [key for key in expected_keys if key not in result]
            completeness = (len(expected_keys) - len(missing_keys)) / len(expected_keys) * 100
            
            self.test_results['evaluators'] = {
                'evaluation_time': evaluation_time,
                'completeness': completeness,
                'missing_keys': missing_keys,
                'result_keys': list(result.keys()),
                'success': True
            }
            
            logger.log_important(f"ğŸ“Š è¯„ä¼°å™¨å¤æµ‹ç»“æœ:")
            logger.log_important(f"   è¯„ä¼°æ—¶é—´: {evaluation_time:.2f}ç§’")
            logger.log_important(f"   ç»“æœå®Œæ•´æ€§: {completeness:.1f}%")
            logger.log_important(f"   ç¼ºå¤±é”®: {missing_keys if missing_keys else 'æ— '}")
            
            if completeness >= 90:
                logger.log_success("âœ… è¯„ä¼°å™¨å¤æµ‹é€šè¿‡")
            else:
                logger.log_warning(f"âš ï¸ è¯„ä¼°å™¨ç»“æœä¸å®Œæ•´")
                
        except Exception as e:
            logger.log_error(f"âŒ è¯„ä¼°å™¨å¤æµ‹å¤±è´¥: {e}")
            self.test_results['evaluators'] = {
                'success': False,
                'error': str(e)
            }
    
    async def _retest_visualization(self):
        """å¤æµ‹å¯è§†åŒ–åŠŸèƒ½"""
        logger.log_important("\nğŸ“ˆ 6. å¯è§†åŒ–åŠŸèƒ½å¤æµ‹")
        logger.log_important("-" * 40)
        
        try:
            viz_manager = EvolutionVisualizer()
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = {
                'generations': list(range(1, 6)),
                'best_fitness': [0.02, 0.03, 0.04, 0.05, 0.06],
                'avg_fitness': [0.015, 0.025, 0.035, 0.045, 0.055],
                'diversity': [0.8, 0.7, 0.6, 0.5, 0.4]
            }
            
            # æµ‹è¯•è¿›åŒ–æ›²çº¿ç”Ÿæˆ
            evolution_plot_path = viz_manager.plot_evolution_curves(test_data)
            
            # æµ‹è¯•å¤šæ ·æ€§çƒ­åŠ›å›¾
            diversity_data = np.random.rand(5, 5)
            diversity_plot_path = viz_manager.plot_diversity_heatmap(diversity_data)
            
            # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
            report_data = {
                'total_generations': 5,
                'final_best_fitness': 0.06,
                'improvement_rate': 200.0,
                'diversity_trend': 'decreasing'
            }
            report_path = viz_manager.generate_evolution_report(report_data)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
            files_generated = []
            for path in [evolution_plot_path, diversity_plot_path, report_path]:
                if path and os.path.exists(path):
                    files_generated.append(os.path.basename(path))
            
            success_rate = len(files_generated) / 3 * 100
            
            self.test_results['visualization'] = {
                'success_rate': success_rate,
                'files_generated': files_generated,
                'evolution_plot': evolution_plot_path,
                'diversity_plot': diversity_plot_path,
                'report_path': report_path,
                'success': True
            }
            
            logger.log_important(f"ğŸ“Š å¯è§†åŒ–åŠŸèƒ½å¤æµ‹ç»“æœ:")
            logger.log_important(f"   æˆåŠŸç‡: {success_rate:.1f}%")
            logger.log_important(f"   ç”Ÿæˆæ–‡ä»¶: {files_generated}")
            
            if success_rate >= 80:
                logger.log_success("âœ… å¯è§†åŒ–åŠŸèƒ½å¤æµ‹é€šè¿‡")
            else:
                logger.log_warning(f"âš ï¸ å¯è§†åŒ–åŠŸèƒ½éœ€è¦æ”¹è¿›")
                
        except Exception as e:
            logger.log_error(f"âŒ å¯è§†åŒ–åŠŸèƒ½å¤æµ‹å¤±è´¥: {e}")
            self.test_results['visualization'] = {
                'success': False,
                'error': str(e)
            }
    
    async def _verify_optimization_effects(self):
        """éªŒè¯ä¼˜åŒ–æ•ˆæœ"""
        logger.log_important("\nğŸ” 7. ä¼˜åŒ–æ•ˆæœéªŒè¯")
        logger.log_important("-" * 40)
        
        # éªŒè¯æ¨ç†åˆ†æ•°ç›®æ ‡
        reasoning_score = self.test_results.get('reasoning_capabilities', {}).get('avg_score', 0)
        target_achieved = reasoning_score >= 0.1
        
        # éªŒè¯ç³»ç»Ÿç¨³å®šæ€§
        stability_rate = self.test_results.get('system_stability', {}).get('stability_rate', 0)
        stability_achieved = stability_rate >= 90
        
        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        performance_success = self.test_results.get('performance_metrics', {}).get('success_rate', 0)
        performance_achieved = performance_success >= 80
        
        # éªŒè¯è¿›åŒ–ç®—æ³•
        evolution_success = self.test_results.get('evolution_algorithm', {}).get('success', False)
        
        # éªŒè¯è¯„ä¼°å™¨
        evaluator_success = self.test_results.get('evaluators', {}).get('success', False)
        
        # éªŒè¯å¯è§†åŒ–
        viz_success = self.test_results.get('visualization', {}).get('success', False)
        
        self.optimization_verification = {
            'reasoning_target': target_achieved,
            'stability_target': stability_achieved,
            'performance_target': performance_achieved,
            'evolution_working': evolution_success,
            'evaluator_working': evaluator_success,
            'visualization_working': viz_success,
            'overall_success': all([
                target_achieved, stability_achieved, performance_achieved,
                evolution_success, evaluator_success, viz_success
            ])
        }
        
        logger.log_important(f"ğŸ“Š ä¼˜åŒ–æ•ˆæœéªŒè¯ç»“æœ:")
        logger.log_important(f"   æ¨ç†åˆ†æ•°ç›®æ ‡: {'âœ…' if target_achieved else 'âŒ'} ({reasoning_score:.4f})")
        logger.log_important(f"   ç³»ç»Ÿç¨³å®šæ€§: {'âœ…' if stability_achieved else 'âŒ'} ({stability_rate:.1f}%)")
        logger.log_important(f"   æ€§èƒ½æŒ‡æ ‡: {'âœ…' if performance_achieved else 'âŒ'} ({performance_success:.1f}%)")
        logger.log_important(f"   è¿›åŒ–ç®—æ³•: {'âœ…' if evolution_success else 'âŒ'}")
        logger.log_important(f"   è¯„ä¼°å™¨: {'âœ…' if evaluator_success else 'âŒ'}")
        logger.log_important(f"   å¯è§†åŒ–: {'âœ…' if viz_success else 'âŒ'}")
        
        if self.optimization_verification['overall_success']:
            logger.log_success("ğŸ‰ æ‰€æœ‰ä¼˜åŒ–ç›®æ ‡å‡å·²è¾¾æˆï¼")
        else:
            logger.log_warning("âš ï¸ éƒ¨åˆ†ä¼˜åŒ–ç›®æ ‡å°šæœªè¾¾æˆï¼Œéœ€è¦ç»§ç»­æ”¹è¿›")
    
    def _generate_retest_report(self):
        """ç”Ÿæˆå¤æµ‹æŠ¥å‘Š"""
        logger.log_important("\nğŸ“‹ å…¨é¢ç³»ç»Ÿå¤æµ‹æŠ¥å‘Š")
        logger.log_important("=" * 60)
        
        # ç»Ÿè®¡æ€»ä½“ç»“æœ
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results.values() 
                             if isinstance(result, dict) and result.get('success', True))
        
        overall_success_rate = successful_tests / total_tests * 100
        
        logger.log_important(f"ğŸ“Š å¤æµ‹æ€»ä½“ç»“æœ:")
        logger.log_important(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.log_important(f"   æˆåŠŸæµ‹è¯•: {successful_tests}")
        logger.log_important(f"   æˆåŠŸç‡: {overall_success_rate:.1f}%")
        
        # è¯¦ç»†ç»“æœ
        logger.log_important(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        
        for test_name, result in self.test_results.items():
            if isinstance(result, dict):
                success = result.get('success', True)
                status = "âœ…" if success else "âŒ"
                logger.log_important(f"   {status} {test_name}")
                
                # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
                if test_name == 'reasoning_capabilities':
                    score = result.get('avg_score', 0)
                    target_achieved = result.get('target_achieved', False)
                    logger.log_important(f"      æ¨ç†åˆ†æ•°: {score:.4f} {'âœ…' if target_achieved else 'âŒ'}")
                
                elif test_name == 'system_stability':
                    stability_rate = result.get('stability_rate', 0)
                    logger.log_important(f"      ç¨³å®šæ€§: {stability_rate:.1f}%")
                
                elif test_name == 'performance_metrics':
                    success_rate = result.get('success_rate', 0)
                    logger.log_important(f"      æˆåŠŸç‡: {success_rate:.1f}%")
        
        # ä¼˜åŒ–æ•ˆæœæ€»ç»“
        if self.optimization_verification:
            logger.log_important(f"\nğŸ¯ ä¼˜åŒ–æ•ˆæœæ€»ç»“:")
            overall_success = self.optimization_verification.get('overall_success', False)
            logger.log_important(f"   æ•´ä½“ä¼˜åŒ–æ•ˆæœ: {'âœ… ä¼˜ç§€' if overall_success else 'âš ï¸ éœ€æ”¹è¿›'}")
        
        # æœ€ç»ˆè¯„ä¼°
        if overall_success_rate >= 90:
            logger.log_success("ğŸ‰ å…¨é¢ç³»ç»Ÿå¤æµ‹é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œä¼˜ç§€")
        elif overall_success_rate >= 80:
            logger.log_important("âœ… å…¨é¢ç³»ç»Ÿå¤æµ‹åŸºæœ¬é€šè¿‡ï¼Œéƒ¨åˆ†åŠŸèƒ½éœ€è¦æ”¹è¿›")
        else:
            logger.log_warning("âš ï¸ å…¨é¢ç³»ç»Ÿå¤æµ‹å‘ç°é—®é¢˜ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›")

async def main():
    """ä¸»å‡½æ•°"""
    logger.log_important("=== å…¨é¢ç³»ç»Ÿå¤æµ‹ ===")
    
    # åˆ›å»ºå¤æµ‹å™¨
    retester = ComprehensiveRetest()
    
    # è¿è¡Œå…¨é¢å¤æµ‹
    results = await retester.run_comprehensive_retest()
    
    logger.log_important(f"\nğŸ‰ å…¨é¢ç³»ç»Ÿå¤æµ‹å®Œæˆï¼")
    logger.log_important(f"å¤æµ‹ç»“æœå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")

if __name__ == "__main__":
    asyncio.run(main()) 