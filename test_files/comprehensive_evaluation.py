#!/usr/bin/env python3
"""
å…¨é¢æ¨¡å‹è¯„ä¼°æµ‹è¯•
æ£€æŸ¥æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å’Œæ–°å¢åŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio
import numpy as np
import torch
import time
from typing import Dict, List, Any
from models.advanced_reasoning_net import AdvancedReasoningNet
from evaluators.enhanced_evaluator import EnhancedEvaluator
from evolution.advanced_evolution import AdvancedEvolution, MultiObjectiveAdvancedEvolution
from utils.visualization import EvolutionVisualizer
from config.optimized_logging import setup_optimized_logging

logger = setup_optimized_logging()

class ComprehensiveEvaluator:
    """å…¨é¢è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.test_results = {}
        self.performance_metrics = {}
        
    async def run_comprehensive_evaluation(self):
        """è¿è¡Œå…¨é¢è¯„ä¼°"""
        logger.log_important("ğŸ” ğŸš€ å¼€å§‹å…¨é¢æ¨¡å‹è¯„ä¼°")
        logger.log_important("=" * 60)
        
        # 1. æ¨¡å‹æ¶æ„æµ‹è¯•
        await self._test_model_architecture()
        
        # 2. æ¨ç†èƒ½åŠ›æµ‹è¯•
        await self._test_reasoning_capabilities()
        
        # 3. å¼‚æ„ç»“æ„æµ‹è¯•
        await self._test_heterogeneous_structures()
        
        # 4. è¿›åŒ–ç®—æ³•æµ‹è¯•
        await self._test_evolution_algorithm()
        
        # 5. è¯„ä¼°å™¨æµ‹è¯•
        await self._test_evaluators()
        
        # 6. å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•
        await self._test_visualization()
        
        # 7. æ€§èƒ½åŸºå‡†æµ‹è¯•
        await self._test_performance_benchmarks()
        
        # 8. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        self._generate_evaluation_report()
        
        return self.test_results
    
    async def _test_model_architecture(self):
        """æµ‹è¯•æ¨¡å‹æ¶æ„"""
        logger.log_important("ğŸ”§ 1. æ¨¡å‹æ¶æ„æµ‹è¯•")
        
        try:
            # æµ‹è¯•åŸºæœ¬æ¨¡å‹åˆ›å»º
            model = AdvancedReasoningNet(
                input_size=4,
                hidden_size=256,
                reasoning_layers=5,
                attention_heads=8,
                memory_size=20,
                reasoning_types=10
            )
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            test_input = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)
            output = model(test_input)
            
            # æ£€æŸ¥è¾“å‡ºç»“æ„
            expected_keys = ['comprehensive_reasoning', 'reasoning_chain', 'symbolic_expression']
            for key in expected_keys:
                if key in output:
                    logger.log_success(f"âœ… è¾“å‡ºé”® '{key}' å­˜åœ¨")
                else:
                    logger.log_warning(f"âš ï¸ è¾“å‡ºé”® '{key}' ç¼ºå¤±")
            
            # æµ‹è¯•æ¨ç†é“¾
            reasoning_chain = model.get_reasoning_chain()
            if reasoning_chain:
                logger.log_success(f"âœ… æ¨ç†é“¾ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(reasoning_chain)}")
            else:
                logger.log_warning("âš ï¸ æ¨ç†é“¾ä¸ºç©º")
            
            # æµ‹è¯•ç¬¦å·è¡¨è¾¾å¼
            symbolic_expr = model.extract_symbolic(use_llm=False)
            if symbolic_expr:
                logger.log_success(f"âœ… ç¬¦å·è¡¨è¾¾å¼ç”ŸæˆæˆåŠŸ: {symbolic_expr}")
            else:
                logger.log_warning("âš ï¸ ç¬¦å·è¡¨è¾¾å¼ä¸ºç©º")
            
            self.test_results['model_architecture'] = {
                'status': 'PASS',
                'output_keys': list(output.keys()),
                'reasoning_chain_length': len(reasoning_chain) if reasoning_chain else 0,
                'symbolic_expression': symbolic_expr
            }
            
        except Exception as e:
            logger.log_error(f"âŒ æ¨¡å‹æ¶æ„æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['model_architecture'] = {'status': 'FAIL', 'error': str(e)}
    
    async def _test_reasoning_capabilities(self):
        """æµ‹è¯•æ¨ç†èƒ½åŠ›"""
        logger.log_important("ğŸ§  2. æ¨ç†èƒ½åŠ›æµ‹è¯•")
        
        try:
            model = AdvancedReasoningNet(
                input_size=4,
                hidden_size=256,
                reasoning_layers=5,
                attention_heads=8,
                memory_size=20,
                reasoning_types=10
            )
            
            # æµ‹è¯•ä¸åŒç±»å‹çš„è¾“å…¥
            test_cases = [
                ([1, 2, 3, 4], "åŸºç¡€æ¨ç†"),
                ([2, 4, 6, 8], "å¶æ•°åºåˆ—"),
                ([1, 3, 5, 7], "å¥‡æ•°åºåˆ—"),
                ([1, 4, 9, 16], "å¹³æ–¹åºåˆ—")
            ]
            
            reasoning_results = {}
            for inputs, description in test_cases:
                test_input = torch.tensor([inputs], dtype=torch.float32)
                output = model(test_input)
                
                comprehensive_score = output.get('comprehensive_reasoning', torch.tensor(0.0))
                if isinstance(comprehensive_score, torch.Tensor):
                    score = comprehensive_score.mean().item()
                else:
                    score = float(comprehensive_score)
                
                reasoning_results[description] = score
                logger.log_important(f"ğŸ”” {description}: {score:.3f}")
            
            self.test_results['reasoning_capabilities'] = {
                'status': 'PASS',
                'test_cases': reasoning_results,
                'average_score': np.mean(list(reasoning_results.values()))
            }
            
        except Exception as e:
            logger.log_error(f"âŒ æ¨ç†èƒ½åŠ›æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['reasoning_capabilities'] = {'status': 'FAIL', 'error': str(e)}
    
    async def _test_heterogeneous_structures(self):
        """æµ‹è¯•å¼‚æ„ç»“æ„"""
        logger.log_important("ğŸ—ï¸ 3. å¼‚æ„ç»“æ„æµ‹è¯•")
        
        try:
            # åˆ›å»ºä¸åŒç»“æ„çš„æ¨¡å‹
            structures = [
                (128, 4, 4, 15, 8),
                (256, 5, 8, 20, 10),
                (384, 6, 12, 25, 12),
                (512, 7, 16, 30, 15)
            ]
            
            models = []
            for hidden_size, layers, heads, memory, types in structures:
                # ç¡®ä¿hidden_sizeèƒ½è¢«headsæ•´é™¤
                adjusted_hidden = (hidden_size // heads) * heads
                model = AdvancedReasoningNet(
                    input_size=4,
                    hidden_size=adjusted_hidden,
                    reasoning_layers=layers,
                    attention_heads=heads,
                    memory_size=memory,
                    reasoning_types=types
                )
                models.append(model)
            
            # æµ‹è¯•å¼‚æ„ç§ç¾¤
            test_input = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)
            outputs = []
            
            for i, model in enumerate(models):
                try:
                    output = model(test_input)
                    comprehensive_score = output.get('comprehensive_reasoning', torch.tensor(0.0))
                    if isinstance(comprehensive_score, torch.Tensor):
                        score = comprehensive_score.mean().item()
                    else:
                        score = float(comprehensive_score)
                    outputs.append(score)
                    logger.log_success(f"âœ… æ¨¡å‹ {i+1} (ç»“æ„{structures[i]}): {score:.3f}")
                except Exception as e:
                    logger.log_warning(f"âš ï¸ æ¨¡å‹ {i+1} å¤±è´¥: {e}")
                    outputs.append(0.0)
            
            self.test_results['heterogeneous_structures'] = {
                'status': 'PASS',
                'structures_tested': len(structures),
                'successful_models': len([o for o in outputs if o > 0]),
                'average_score': np.mean(outputs),
                'structure_diversity': len(set(str(s) for s in structures))
            }
            
        except Exception as e:
            logger.log_error(f"âŒ å¼‚æ„ç»“æ„æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['heterogeneous_structures'] = {'status': 'FAIL', 'error': str(e)}
    
    async def _test_evolution_algorithm(self):
        """æµ‹è¯•è¿›åŒ–ç®—æ³•"""
        logger.log_important("ğŸ”„ 4. è¿›åŒ–ç®—æ³•æµ‹è¯•")
        
        try:
            # åˆ›å»ºåŒæ„ç§ç¾¤
            population = []
            for i in range(4):
                model = AdvancedReasoningNet(
                    input_size=4,
                    hidden_size=256,
                    reasoning_layers=5,
                    attention_heads=8,
                    memory_size=20,
                    reasoning_types=10
                )
                population.append(model)
            
            # åˆ›å»ºè¯„ä¼°å™¨
            evaluator = EnhancedEvaluator()
            
            # æµ‹è¯•è¿›åŒ–ç®—æ³•
            evolution = AdvancedEvolution(
                population_size=4,
                mutation_rate=0.1,
                crossover_rate=0.8,
                elite_size=1
            )
            
            # æ‰§è¡Œè¿›åŒ–
            start_time = time.time()
            evolved_population = evolution.evolve(
                population=population,
                evaluator=evaluator,
                generations=2
            )
            evolution_time = time.time() - start_time
            
            logger.log_success(f"âœ… è¿›åŒ–ç®—æ³•æ‰§è¡ŒæˆåŠŸ")
            logger.log_important(f"ğŸ”” è¿›åŒ–æ—¶é—´: {evolution_time:.2f}ç§’")
            logger.log_important(f"ğŸ”” æœ€ç»ˆç§ç¾¤å¤§å°: {len(evolved_population)}")
            
            self.test_results['evolution_algorithm'] = {
                'status': 'PASS',
                'evolution_time': evolution_time,
                'final_population_size': len(evolved_population),
                'generations_completed': 2
            }
            
        except Exception as e:
            logger.log_error(f"âŒ è¿›åŒ–ç®—æ³•æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['evolution_algorithm'] = {'status': 'FAIL', 'error': str(e)}
    
    async def _test_evaluators(self):
        """æµ‹è¯•è¯„ä¼°å™¨"""
        logger.log_important("ğŸ“Š 5. è¯„ä¼°å™¨æµ‹è¯•")
        
        try:
            model = AdvancedReasoningNet(
                input_size=4,
                hidden_size=256,
                reasoning_layers=5,
                attention_heads=8,
                memory_size=20,
                reasoning_types=10
            )
            
            evaluator = EnhancedEvaluator()
            
            # æµ‹è¯•å¢å¼ºè¯„ä¼°
            start_time = time.time()
            evaluation_result = await evaluator.evaluate_enhanced_reasoning(
                model=model, 
                max_tasks=10
            )
            evaluation_time = time.time() - start_time
            
            logger.log_success(f"âœ… å¢å¼ºè¯„ä¼°å™¨æµ‹è¯•æˆåŠŸ")
            logger.log_important(f"ğŸ”” è¯„ä¼°æ—¶é—´: {evaluation_time:.2f}ç§’")
            logger.log_important(f"ğŸ”” è¯„ä¼°ç»“æœ: {evaluation_result}")
            
            # æ£€æŸ¥è¯„ä¼°ç»“æœå®Œæ•´æ€§
            expected_metrics = [
                'nested_reasoning', 'symbolic_induction', 'graph_reasoning',
                'multi_step_chain', 'logical_chain', 'abstract_concept',
                'creative_reasoning', 'symbolic_expression', 'comprehensive_reasoning'
            ]
            
            missing_metrics = [m for m in expected_metrics if m not in evaluation_result]
            if missing_metrics:
                logger.log_warning(f"âš ï¸ ç¼ºå¤±è¯„ä¼°æŒ‡æ ‡: {missing_metrics}")
            else:
                logger.log_success("âœ… æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡å®Œæ•´")
            
            self.test_results['evaluators'] = {
                'status': 'PASS',
                'evaluation_time': evaluation_time,
                'metrics_count': len(evaluation_result),
                'comprehensive_score': evaluation_result.get('comprehensive_reasoning', 0.0),
                'missing_metrics': missing_metrics
            }
            
        except Exception as e:
            logger.log_error(f"âŒ è¯„ä¼°å™¨æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['evaluators'] = {'status': 'FAIL', 'error': str(e)}
    
    async def _test_visualization(self):
        """æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½"""
        logger.log_important("ğŸ“ˆ 6. å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•")
        
        try:
            # åˆ›å»ºå¯è§†åŒ–å™¨
            visualizer = EvolutionVisualizer()
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            population = []
            for i in range(3):
                model = AdvancedReasoningNet(
                    input_size=4,
                    hidden_size=256,
                    reasoning_layers=5,
                    attention_heads=8,
                    memory_size=20,
                    reasoning_types=10
                )
                population.append(model)
            
            # è®°å½•æµ‹è¯•æ•°æ®
            visualizer.record_generation(
                generation=1,
                population=population,
                fitness_scores=[0.1, 0.2, 0.3],
                diversity=0.5,
                best_fitness=0.3,
                avg_fitness=0.2
            )
            
            visualizer.record_generation(
                generation=2,
                population=population,
                fitness_scores=[0.2, 0.3, 0.4],
                diversity=0.6,
                best_fitness=0.4,
                avg_fitness=0.3
            )
            
            # ç”Ÿæˆå¯è§†åŒ–
            curves_file = visualizer.plot_evolution_curves()
            heatmap_file = visualizer.plot_diversity_heatmap()
            report_file = visualizer.generate_evolution_report()
            data_file = visualizer.save_visualization_data()
            
            logger.log_success("âœ… å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
            logger.log_important(f"ğŸ”” ç”Ÿæˆæ–‡ä»¶:")
            logger.log_important(f"  ğŸ“Š {curves_file}")
            logger.log_important(f"  ğŸ“Š {heatmap_file}")
            logger.log_important(f"  ğŸ“Š {report_file}")
            logger.log_important(f"  ğŸ“Š {data_file}")
            
            self.test_results['visualization'] = {
                'status': 'PASS',
                'files_generated': [curves_file, heatmap_file, report_file, data_file],
                'generations_recorded': 2
            }
            
        except Exception as e:
            logger.log_error(f"âŒ å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['visualization'] = {'status': 'FAIL', 'error': str(e)}
    
    async def _test_performance_benchmarks(self):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        logger.log_important("âš¡ 7. æ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        try:
            # æ¨¡å‹æ¨ç†æ€§èƒ½æµ‹è¯•
            model = AdvancedReasoningNet(
                input_size=4,
                hidden_size=256,
                reasoning_layers=5,
                attention_heads=8,
                memory_size=20,
                reasoning_types=10
            )
            
            test_input = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)
            
            # é¢„çƒ­
            for _ in range(10):
                _ = model(test_input)
            
            # æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            for _ in range(100):
                _ = model(test_input)
            inference_time = (time.time() - start_time) / 100
            
            logger.log_success(f"âœ… æ¨ç†æ€§èƒ½æµ‹è¯•å®Œæˆ")
            logger.log_important(f"ğŸ”” å¹³å‡æ¨ç†æ—¶é—´: {inference_time*1000:.2f}æ¯«ç§’")
            
            # å†…å­˜ä½¿ç”¨æµ‹è¯•
            import psutil
            process = psutil.Process()
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # åˆ›å»ºå¤šä¸ªæ¨¡å‹æµ‹è¯•å†…å­˜
            models = []
            for _ in range(10):
                model = AdvancedReasoningNet(
                    input_size=4,
                    hidden_size=256,
                    reasoning_layers=5,
                    attention_heads=8,
                    memory_size=20,
                    reasoning_types=10
                )
                models.append(model)
            
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            memory_usage = memory_after - memory_before
            
            logger.log_important(f"ğŸ”” å†…å­˜ä½¿ç”¨: {memory_usage:.2f}MB")
            
            self.test_results['performance_benchmarks'] = {
                'status': 'PASS',
                'inference_time_ms': inference_time * 1000,
                'memory_usage_mb': memory_usage,
                'models_created': 10
            }
            
        except Exception as e:
            logger.log_error(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['performance_benchmarks'] = {'status': 'FAIL', 'error': str(e)}
    
    def _generate_evaluation_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        logger.log_important("ğŸ“‹ 8. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
        logger.log_important("=" * 60)
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result.get('status') == 'PASS')
        failed_tests = total_tests - passed_tests
        
        logger.log_important(f"ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
        logger.log_important(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.log_important(f"  é€šè¿‡æµ‹è¯•: {passed_tests}")
        logger.log_important(f"  å¤±è´¥æµ‹è¯•: {failed_tests}")
        logger.log_important(f"  æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
        
        # è¯¦ç»†ç»“æœ
        logger.log_important(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, result in self.test_results.items():
            status = result.get('status', 'UNKNOWN')
            if status == 'PASS':
                logger.log_success(f"  âœ… {test_name}: PASS")
            else:
                error = result.get('error', 'Unknown error')
                logger.log_error(f"  âŒ {test_name}: FAIL - {error}")
        
        # æ€§èƒ½æŒ‡æ ‡æ±‡æ€»
        if 'performance_benchmarks' in self.test_results and self.test_results['performance_benchmarks']['status'] == 'PASS':
            perf = self.test_results['performance_benchmarks']
            logger.log_important(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
            logger.log_important(f"  æ¨ç†æ—¶é—´: {perf['inference_time_ms']:.2f}ms")
            logger.log_important(f"  å†…å­˜ä½¿ç”¨: {perf['memory_usage_mb']:.2f}MB")
        
        # æ¨ç†èƒ½åŠ›æ±‡æ€»
        if 'reasoning_capabilities' in self.test_results and self.test_results['reasoning_capabilities']['status'] == 'PASS':
            reasoning = self.test_results['reasoning_capabilities']
            logger.log_important(f"\nğŸ§  æ¨ç†èƒ½åŠ›:")
            logger.log_important(f"  å¹³å‡æ¨ç†åˆ†æ•°: {reasoning['average_score']:.3f}")
        
        logger.log_important("=" * 60)
        
        if failed_tests == 0:
            logger.log_success("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        else:
            logger.log_warning(f"âš ï¸ {failed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

async def main():
    """ä¸»å‡½æ•°"""
    evaluator = ComprehensiveEvaluator()
    await evaluator.run_comprehensive_evaluation()

if __name__ == "__main__":
    asyncio.run(main()) 