#!/usr/bin/env python3
"""
å¯è§†åŒ–æ•°æ®ä¼˜åŒ–åˆ†æè„šæœ¬
åˆ†æå½“å‰å›¾è¡¨æ•°æ®çš„é—®é¢˜å¹¶æå‡ºä¼˜åŒ–æ–¹æ¡ˆ
"""
import os
import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Any
import glob

class VisualizationOptimizationAnalyzer:
    """å¯è§†åŒ–æ•°æ®ä¼˜åŒ–åˆ†æå™¨"""
    
    def __init__(self):
        self.plots_dir = Path("evolution_plots")
        self.analysis_results = {}
        
    def analyze_current_visualization_data(self):
        """åˆ†æå½“å‰å¯è§†åŒ–æ•°æ®çš„é—®é¢˜"""
        print("ğŸ” å¯è§†åŒ–æ•°æ®ä¼˜åŒ–åˆ†æ")
        print("=" * 50)
        
        # 1. åˆ†ææ–‡ä»¶æ•°é‡å’Œå¤§å°
        self._analyze_file_statistics()
        
        # 2. åˆ†ææ•°æ®è´¨é‡é—®é¢˜
        self._analyze_data_quality()
        
        # 3. åˆ†æå­˜å‚¨æ•ˆç‡
        self._analyze_storage_efficiency()
        
        # 4. åˆ†æç”Ÿæˆé¢‘ç‡
        self._analyze_generation_frequency()
        
        # 5. æå‡ºä¼˜åŒ–æ–¹æ¡ˆ
        self._propose_optimizations()
        
        return self.analysis_results
    
    def _analyze_file_statistics(self):
        """åˆ†ææ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡åˆ†æ:")
        
        # ç»Ÿè®¡æ–‡ä»¶ç±»å‹å’Œå¤§å°
        file_stats = {
            'png_files': [],
            'json_files': [],
            'total_size': 0
        }
        
        if self.plots_dir.exists():
            for file_path in self.plots_dir.glob("*"):
                if file_path.is_file():
                    file_size = file_path.stat().st_size
                    file_stats['total_size'] += file_size
                    
                    if file_path.suffix == '.png':
                        file_stats['png_files'].append({
                            'name': file_path.name,
                            'size': file_size,
                            'size_mb': file_size / (1024 * 1024)
                        })
                    elif file_path.suffix == '.json':
                        file_stats['json_files'].append({
                            'name': file_path.name,
                            'size': file_size,
                            'size_mb': file_size / (1024 * 1024)
                        })
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        png_count = len(file_stats['png_files'])
        json_count = len(file_stats['json_files'])
        total_size_mb = file_stats['total_size'] / (1024 * 1024)
        
        print(f"   PNGæ–‡ä»¶æ•°é‡: {png_count}")
        print(f"   JSONæ–‡ä»¶æ•°é‡: {json_count}")
        print(f"   æ€»æ–‡ä»¶å¤§å°: {total_size_mb:.2f} MB")
        
        if png_count > 0:
            avg_png_size = sum(f['size_mb'] for f in file_stats['png_files']) / png_count
            print(f"   å¹³å‡PNGæ–‡ä»¶å¤§å°: {avg_png_size:.2f} MB")
        
        if json_count > 0:
            avg_json_size = sum(f['size_mb'] for f in file_stats['json_files']) / json_count
            print(f"   å¹³å‡JSONæ–‡ä»¶å¤§å°: {avg_json_size:.2f} MB")
        
        self.analysis_results['file_statistics'] = {
            'png_count': png_count,
            'json_count': json_count,
            'total_size_mb': total_size_mb,
            'avg_png_size_mb': avg_png_size if png_count > 0 else 0,
            'avg_json_size_mb': avg_json_size if json_count > 0 else 0
        }
    
    def _analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡é—®é¢˜"""
        print("\nğŸ” æ•°æ®è´¨é‡åˆ†æ:")
        
        quality_issues = []
        
        # æ£€æŸ¥JSONæ–‡ä»¶ä¸­çš„æ•°æ®è´¨é‡
        json_files = list(self.plots_dir.glob("*.json"))
        if json_files:
            sample_file = json_files[0]
            try:
                with open(sample_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æ£€æŸ¥NaNå€¼
                nan_count = 0
                total_values = 0
                
                def count_nans(obj):
                    nonlocal nan_count, total_values
                    if isinstance(obj, dict):
                        for value in obj.values():
                            count_nans(value)
                    elif isinstance(obj, list):
                        for item in obj:
                            count_nans(item)
                    elif isinstance(obj, (int, float)):
                        total_values += 1
                        if np.isnan(obj):
                            nan_count += 1
                
                count_nans(data)
                
                if total_values > 0:
                    nan_percentage = (nan_count / total_values) * 100
                    print(f"   NaNå€¼æ¯”ä¾‹: {nan_percentage:.1f}% ({nan_count}/{total_values})")
                    
                    if nan_percentage > 5:
                        quality_issues.append(f"NaNå€¼è¿‡å¤š: {nan_percentage:.1f}%")
                
                # æ£€æŸ¥æ•°æ®é‡å¤
                if 'evolution_history' in data:
                    history = data['evolution_history']
                    if len(history) > 1:
                        # æ£€æŸ¥è¿ç»­ä»£æ•°çš„æ•°æ®æ˜¯å¦é‡å¤
                        duplicate_count = 0
                        for i in range(1, len(history)):
                            if (history[i]['best_fitness'] == history[i-1]['best_fitness'] and
                                history[i]['avg_fitness'] == history[i-1]['avg_fitness']):
                                duplicate_count += 1
                        
                        duplicate_percentage = (duplicate_count / (len(history) - 1)) * 100
                        print(f"   æ•°æ®é‡å¤æ¯”ä¾‹: {duplicate_percentage:.1f}%")
                        
                        if duplicate_percentage > 20:
                            quality_issues.append(f"æ•°æ®é‡å¤è¿‡å¤š: {duplicate_percentage:.1f}%")
                
                # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
                required_fields = ['evolution_history', 'diversity_history', 'fitness_history']
                missing_fields = [field for field in required_fields if field not in data]
                if missing_fields:
                    quality_issues.append(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
                
            except Exception as e:
                quality_issues.append(f"JSONè§£æé”™è¯¯: {e}")
        
        if quality_issues:
            print("   âŒ å‘ç°æ•°æ®è´¨é‡é—®é¢˜:")
            for issue in quality_issues:
                print(f"     - {issue}")
        else:
            print("   âœ… æ•°æ®è´¨é‡è‰¯å¥½")
        
        self.analysis_results['data_quality'] = {
            'issues': quality_issues,
            'nan_percentage': nan_percentage if 'nan_percentage' in locals() else 0,
            'duplicate_percentage': duplicate_percentage if 'duplicate_percentage' in locals() else 0
        }
    
    def _analyze_storage_efficiency(self):
        """åˆ†æå­˜å‚¨æ•ˆç‡"""
        print("\nğŸ’¾ å­˜å‚¨æ•ˆç‡åˆ†æ:")
        
        efficiency_issues = []
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¿‡å¤§
        png_files = list(self.plots_dir.glob("*.png"))
        if png_files:
            large_files = [f for f in png_files if f.stat().st_size > 200 * 1024]  # 200KB
            if large_files:
                efficiency_issues.append(f"å¤§æ–‡ä»¶è¿‡å¤š: {len(large_files)} ä¸ªæ–‡ä»¶è¶…è¿‡200KB")
                print(f"   âš ï¸  å‘ç° {len(large_files)} ä¸ªå¤§æ–‡ä»¶ (>200KB)")
        
        # æ£€æŸ¥JSONæ–‡ä»¶å‹ç¼©
        json_files = list(self.plots_dir.glob("*.json"))
        if json_files:
            total_json_size = sum(f.stat().st_size for f in json_files)
            print(f"   JSONæ–‡ä»¶æ€»å¤§å°: {total_json_size / 1024:.1f} KB")
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å‹ç¼©
            if total_json_size > 100 * 1024:  # 100KB
                efficiency_issues.append("JSONæ–‡ä»¶è¿‡å¤§ï¼Œå»ºè®®å‹ç¼©")
        
        # æ£€æŸ¥æ–‡ä»¶å‘½åè§„èŒƒ
        files = list(self.plots_dir.glob("*"))
        timestamp_files = [f for f in files if '_20250725_' in f.name]
        if len(timestamp_files) > 50:
            efficiency_issues.append("æ–‡ä»¶æ•°é‡è¿‡å¤šï¼Œå»ºè®®æ¸…ç†æ—§æ–‡ä»¶")
            print(f"   âš ï¸  æ—¶é—´æˆ³æ–‡ä»¶è¿‡å¤š: {len(timestamp_files)} ä¸ª")
        
        if efficiency_issues:
            print("   âŒ å­˜å‚¨æ•ˆç‡é—®é¢˜:")
            for issue in efficiency_issues:
                print(f"     - {issue}")
        else:
            print("   âœ… å­˜å‚¨æ•ˆç‡è‰¯å¥½")
        
        self.analysis_results['storage_efficiency'] = {
            'issues': efficiency_issues,
            'large_file_count': len(large_files) if 'large_files' in locals() else 0,
            'timestamp_file_count': len(timestamp_files) if 'timestamp_files' in locals() else 0
        }
    
    def _analyze_generation_frequency(self):
        """åˆ†æç”Ÿæˆé¢‘ç‡"""
        print("\nâ±ï¸ ç”Ÿæˆé¢‘ç‡åˆ†æ:")
        
        # åˆ†ææ–‡ä»¶ç”Ÿæˆæ—¶é—´
        files = list(self.plots_dir.glob("*"))
        if files:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
            files.sort(key=lambda x: x.stat().st_mtime)
            
            # è®¡ç®—ç”Ÿæˆé—´éš”
            intervals = []
            for i in range(1, len(files)):
                interval = files[i].stat().st_mtime - files[i-1].stat().st_mtime
                intervals.append(interval)
            
            if intervals:
                avg_interval = np.mean(intervals)
                min_interval = np.min(intervals)
                max_interval = np.max(intervals)
                
                print(f"   å¹³å‡ç”Ÿæˆé—´éš”: {avg_interval:.1f} ç§’")
                print(f"   æœ€çŸ­ç”Ÿæˆé—´éš”: {min_interval:.1f} ç§’")
                print(f"   æœ€é•¿ç”Ÿæˆé—´éš”: {max_interval:.1f} ç§’")
                
                # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆè¿‡äºé¢‘ç¹
                if min_interval < 10:  # 10ç§’å†…
                    print("   âš ï¸  ç”Ÿæˆè¿‡äºé¢‘ç¹ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
                
                # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆè¿‡äºç¨€ç–
                if avg_interval > 300:  # 5åˆ†é’Ÿ
                    print("   âš ï¸  ç”Ÿæˆé—´éš”è¿‡é•¿ï¼Œå¯èƒ½ä¸¢å¤±é‡è¦æ•°æ®")
        
        self.analysis_results['generation_frequency'] = {
            'avg_interval': avg_interval if 'avg_interval' in locals() else 0,
            'min_interval': min_interval if 'min_interval' in locals() else 0,
            'max_interval': max_interval if 'max_interval' in locals() else 0
        }
    
    def _propose_optimizations(self):
        """æå‡ºä¼˜åŒ–æ–¹æ¡ˆ"""
        print("\nğŸš€ ä¼˜åŒ–æ–¹æ¡ˆå»ºè®®:")
        
        optimizations = []
        
        # 1. æ•°æ®å‹ç¼©ä¼˜åŒ–
        optimizations.append({
            'category': 'æ•°æ®å‹ç¼©',
            'description': 'å‹ç¼©JSONæ•°æ®ï¼Œå‡å°‘å­˜å‚¨ç©ºé—´',
            'implementation': 'ä½¿ç”¨gzipå‹ç¼©JSONæ–‡ä»¶',
            'expected_benefit': 'å‡å°‘50-70%å­˜å‚¨ç©ºé—´'
        })
        
        # 2. å›¾ç‰‡è´¨é‡ä¼˜åŒ–
        optimizations.append({
            'category': 'å›¾ç‰‡è´¨é‡',
            'description': 'ä¼˜åŒ–PNGå›¾ç‰‡è´¨é‡å’Œå¤§å°',
            'implementation': 'è°ƒæ•´DPIå’Œå‹ç¼©å‚æ•°',
            'expected_benefit': 'å‡å°‘30-50%æ–‡ä»¶å¤§å°'
        })
        
        # 3. æ•°æ®æ¸…ç†ä¼˜åŒ–
        optimizations.append({
            'category': 'æ•°æ®æ¸…ç†',
            'description': 'æ¸…ç†NaNå€¼å’Œé‡å¤æ•°æ®',
            'implementation': 'åœ¨æ•°æ®è®°å½•å‰è¿›è¡ŒéªŒè¯å’Œæ¸…ç†',
            'expected_benefit': 'æé«˜æ•°æ®è´¨é‡ï¼Œå‡å°‘å­˜å‚¨'
        })
        
        # 4. ç”Ÿæˆé¢‘ç‡ä¼˜åŒ–
        optimizations.append({
            'category': 'ç”Ÿæˆé¢‘ç‡',
            'description': 'ä¼˜åŒ–å›¾è¡¨ç”Ÿæˆé¢‘ç‡',
            'implementation': 'å®ç°æ™ºèƒ½ç”Ÿæˆç­–ç•¥',
            'expected_benefit': 'å‡å°‘ä¸å¿…è¦çš„æ–‡ä»¶ç”Ÿæˆ'
        })
        
        # 5. æ–‡ä»¶ç®¡ç†ä¼˜åŒ–
        optimizations.append({
            'category': 'æ–‡ä»¶ç®¡ç†',
            'description': 'å®ç°è‡ªåŠ¨æ–‡ä»¶æ¸…ç†',
            'implementation': 'ä¿ç•™æœ€æ–°çš„Nä¸ªæ–‡ä»¶ï¼Œè‡ªåŠ¨åˆ é™¤æ—§æ–‡ä»¶',
            'expected_benefit': 'æ§åˆ¶å­˜å‚¨ç©ºé—´ä½¿ç”¨'
        })
        
        # 6. æ•°æ®æ ¼å¼ä¼˜åŒ–
        optimizations.append({
            'category': 'æ•°æ®æ ¼å¼',
            'description': 'ä¼˜åŒ–æ•°æ®å­˜å‚¨æ ¼å¼',
            'implementation': 'ä½¿ç”¨æ›´é«˜æ•ˆçš„äºŒè¿›åˆ¶æ ¼å¼æˆ–æ•°æ®åº“',
            'expected_benefit': 'æé«˜è¯»å†™æ•ˆç‡'
        })
        
        for i, opt in enumerate(optimizations, 1):
            print(f"   {i}. {opt['category']}: {opt['description']}")
            print(f"      å®ç°: {opt['implementation']}")
            print(f"      é¢„æœŸæ”¶ç›Š: {opt['expected_benefit']}")
            print()
        
        self.analysis_results['optimizations'] = optimizations
    
    def generate_optimization_report(self):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report = {
            'analysis_timestamp': str(np.datetime64('now')),
            'summary': {
                'total_files': self.analysis_results.get('file_statistics', {}).get('png_count', 0) + 
                              self.analysis_results.get('file_statistics', {}).get('json_count', 0),
                'total_size_mb': self.analysis_results.get('file_statistics', {}).get('total_size_mb', 0),
                'quality_issues': len(self.analysis_results.get('data_quality', {}).get('issues', [])),
                'efficiency_issues': len(self.analysis_results.get('storage_efficiency', {}).get('issues', [])),
                'optimization_count': len(self.analysis_results.get('optimizations', []))
            },
            'detailed_analysis': self.analysis_results
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = "visualization_optimization_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file

def main():
    """ä¸»å‡½æ•°"""
    analyzer = VisualizationOptimizationAnalyzer()
    analyzer.analyze_current_visualization_data()
    analyzer.generate_optimization_report()
    
    print("\nâœ… å¯è§†åŒ–æ•°æ®ä¼˜åŒ–åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main() 