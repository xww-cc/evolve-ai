#!/usr/bin/env python3
"""
ä¸“ä¸šæ¨¡å‹è¯„ä¼°æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
"""

import asyncio
import time
import json
import statistics
import numpy as np
from datetime import datetime
from evolution.population import create_initial_population
from evaluators.realworld_evaluator import RealWorldEvaluator
from evaluators.symbolic_evaluator import SymbolicEvaluator
from evolution.nsga2 import evolve_population_nsga2
from config.logging_setup import setup_logging
from config.global_constants import POPULATION_SIZE, NUM_GENERATIONS, LEVEL_DESCRIPTIONS

logger = setup_logging('model_evaluation_report.log')

class ModelEvaluationReport:
    """æ¨¡å‹è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.report_data = {
            'test_info': {},
            'performance_metrics': {},
            'evaluation_results': {},
            'evolution_analysis': {},
            'diversity_metrics': {},
            'recommendations': {}
        }
        
    async def run_comprehensive_evaluation(self):
        """è¿è¡Œå…¨é¢è¯„ä¼°"""
        print("ğŸ”¬ å¼€å§‹ä¸“ä¸šæ¨¡å‹è¯„ä¼°æµ‹è¯•...")
        start_time = time.time()
        
        # 1. åŸºç¡€æ€§èƒ½æµ‹è¯•
        await self._test_basic_performance()
        
        # 2. è¯„ä¼°å™¨æ€§èƒ½æµ‹è¯•
        await self._test_evaluator_performance()
        
        # 3. è¿›åŒ–ç®—æ³•æµ‹è¯•
        await self._test_evolution_performance()
        
        # 4. å¤šæ ·æ€§åˆ†æ
        await self._test_diversity_analysis()
        
        # 5. ç¨³å®šæ€§æµ‹è¯•
        await self._test_stability()
        
        # 6. å¯æ‰©å±•æ€§æµ‹è¯•
        await self._test_scalability()
        
        total_time = time.time() - start_time
        self.report_data['test_info']['total_test_time'] = total_time
        self.report_data['test_info']['test_date'] = datetime.now().isoformat()
        
        print(f"âœ… è¯„ä¼°æµ‹è¯•å®Œæˆ - æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
    async def _test_basic_performance(self):
        """æµ‹è¯•åŸºç¡€æ€§èƒ½"""
        print("ğŸ“Š æµ‹è¯•åŸºç¡€æ€§èƒ½...")
        
        # ç§ç¾¤åˆ›å»ºæ€§èƒ½
        creation_times = []
        for i in range(5):
            start_time = time.time()
            population = create_initial_population(10)
            creation_time = time.time() - start_time
            creation_times.append(creation_time)
            
        avg_creation_time = statistics.mean(creation_times)
        std_creation_time = statistics.stdev(creation_times)
        
        self.report_data['performance_metrics']['population_creation'] = {
            'average_time': avg_creation_time,
            'std_deviation': std_creation_time,
            'min_time': min(creation_times),
            'max_time': max(creation_times),
            'individuals_per_second': 10 / avg_creation_time
        }
        
        print(f"âœ… ç§ç¾¤åˆ›å»º: å¹³å‡{avg_creation_time:.3f}s Â± {std_creation_time:.3f}s")
        
    async def _test_evaluator_performance(self):
        """æµ‹è¯•è¯„ä¼°å™¨æ€§èƒ½"""
        print("ğŸ”§ æµ‹è¯•è¯„ä¼°å™¨æ€§èƒ½...")
        
        population = create_initial_population(15)
        symbolic_evaluator = SymbolicEvaluator()
        realworld_evaluator = RealWorldEvaluator()
        
        # ç¬¦å·è¯„ä¼°æ€§èƒ½
        symbolic_times = []
        symbolic_scores = []
        for individual in population:
            start_time = time.time()
            score = await symbolic_evaluator.evaluate(individual)
            eval_time = time.time() - start_time
            symbolic_times.append(eval_time)
            symbolic_scores.append(score)
            
        # çœŸå®ä¸–ç•Œè¯„ä¼°æ€§èƒ½
        realworld_times = []
        realworld_scores = []
        for individual in population:
            start_time = time.time()
            score = await realworld_evaluator.evaluate(individual)
            eval_time = time.time() - start_time
            realworld_times.append(eval_time)
            realworld_scores.append(score)
            
        self.report_data['evaluation_results']['symbolic'] = {
            'average_score': statistics.mean(symbolic_scores),
            'std_score': statistics.stdev(symbolic_scores),
            'min_score': min(symbolic_scores),
            'max_score': max(symbolic_scores),
            'average_time': statistics.mean(symbolic_times),
            'std_time': statistics.stdev(symbolic_times),
            'individuals_per_second': len(population) / sum(symbolic_times)
        }
        
        self.report_data['evaluation_results']['realworld'] = {
            'average_score': statistics.mean(realworld_scores),
            'std_score': statistics.stdev(realworld_scores),
            'min_score': min(realworld_scores),
            'max_score': max(realworld_scores),
            'average_time': statistics.mean(realworld_times),
            'std_time': statistics.stdev(realworld_times),
            'individuals_per_second': len(population) / sum(realworld_times)
        }
        
        print(f"âœ… ç¬¦å·è¯„ä¼°: å¹³å‡å¾—åˆ†{statistics.mean(symbolic_scores):.3f}, é€Ÿåº¦{len(population)/sum(symbolic_times):.1f}ä¸ªä½“/ç§’")
        print(f"âœ… çœŸå®ä¸–ç•Œè¯„ä¼°: å¹³å‡å¾—åˆ†{statistics.mean(realworld_scores):.3f}, é€Ÿåº¦{len(population)/sum(realworld_times):.1f}ä¸ªä½“/ç§’")
        
    async def _test_evolution_performance(self):
        """æµ‹è¯•è¿›åŒ–ç®—æ³•æ€§èƒ½"""
        print("ğŸ”„ æµ‹è¯•è¿›åŒ–ç®—æ³•æ€§èƒ½...")
        
        evolution_times = []
        diversity_scores = []
        
        for i in range(3):
            population = create_initial_population(12)
            fitness_scores = [(0.8, 0.6), (0.9, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 0.8),
                             (0.8, 0.7), (0.9, 0.6), (0.7, 0.9), (0.8, 0.8), (0.9, 0.7),
                             (0.8, 0.8), (0.9, 0.6)]
            
            start_time = time.time()
            evolved_population = evolve_population_nsga2(population, fitness_scores)
            evolution_time = time.time() - start_time
            evolution_times.append(evolution_time)
            
            # è®¡ç®—å¤šæ ·æ€§
            diversity = self._calculate_diversity(evolved_population)
            diversity_scores.append(diversity)
            
        self.report_data['evolution_analysis'] = {
            'average_evolution_time': statistics.mean(evolution_times),
            'std_evolution_time': statistics.stdev(evolution_times),
            'generations_per_second': 1 / statistics.mean(evolution_times),
            'average_diversity': statistics.mean(diversity_scores),
            'std_diversity': statistics.stdev(diversity_scores)
        }
        
        print(f"âœ… è¿›åŒ–ç®—æ³•: å¹³å‡{statistics.mean(evolution_times):.3f}s, å¤šæ ·æ€§{statistics.mean(diversity_scores):.3f}")
        
    def _calculate_diversity(self, population):
        """è®¡ç®—ç§ç¾¤å¤šæ ·æ€§"""
        if len(population) < 2:
            return 0.0
            
        # ç®€åŒ–çš„å¤šæ ·æ€§è®¡ç®—
        config_hashes = [hash(str(ind.modules_config)) for ind in population]
        unique_configs = len(set(config_hashes))
        return unique_configs / len(population)
        
    async def _test_diversity_analysis(self):
        """æµ‹è¯•å¤šæ ·æ€§åˆ†æ"""
        print("ğŸŒŠ æµ‹è¯•å¤šæ ·æ€§åˆ†æ...")
        
        diversity_metrics = []
        
        for i in range(5):
            population = create_initial_population(10)
            initial_diversity = self._calculate_diversity(population)
            
            fitness_scores = [(0.8, 0.6)] * len(population)
            evolved_population = evolve_population_nsga2(population, fitness_scores)
            final_diversity = self._calculate_diversity(evolved_population)
            
            diversity_metrics.append({
                'initial': initial_diversity,
                'final': final_diversity,
                'change': final_diversity - initial_diversity
            })
            
        self.report_data['diversity_metrics'] = {
            'average_initial_diversity': statistics.mean([m['initial'] for m in diversity_metrics]),
            'average_final_diversity': statistics.mean([m['final'] for m in diversity_metrics]),
            'average_diversity_change': statistics.mean([m['change'] for m in diversity_metrics]),
            'diversity_maintenance_rate': sum(1 for m in diversity_metrics if m['final'] >= 0.5) / len(diversity_metrics)
        }
        
        print(f"âœ… å¤šæ ·æ€§åˆ†æ: åˆå§‹{statistics.mean([m['initial'] for m in diversity_metrics]):.3f}, æœ€ç»ˆ{statistics.mean([m['final'] for m in diversity_metrics]):.3f}")
        
    async def _test_stability(self):
        """æµ‹è¯•ç¨³å®šæ€§"""
        print("ğŸ”’ æµ‹è¯•ç³»ç»Ÿç¨³å®šæ€§...")
        
        stability_results = []
        
        for i in range(3):
            try:
                start_time = time.time()
                
                # å®Œæ•´æµç¨‹æµ‹è¯•
                population = create_initial_population(8)
                symbolic_evaluator = SymbolicEvaluator()
                realworld_evaluator = RealWorldEvaluator()
                
                fitness_scores = []
                for individual in population:
                    symbolic_score = await symbolic_evaluator.evaluate(individual)
                    realworld_score = await realworld_evaluator.evaluate(individual)
                    fitness_scores.append((symbolic_score, realworld_score))
                    
                evolved_population = evolve_population_nsga2(population, fitness_scores)
                
                total_time = time.time() - start_time
                stability_results.append({
                    'success': True,
                    'time': total_time,
                    'population_size': len(evolved_population)
                })
                
            except Exception as e:
                stability_results.append({
                    'success': False,
                    'error': str(e)
                })
                
        success_rate = sum(1 for r in stability_results if r['success']) / len(stability_results)
        avg_time = statistics.mean([r['time'] for r in stability_results if r['success']])
        
        self.report_data['performance_metrics']['stability'] = {
            'success_rate': success_rate,
            'average_execution_time': avg_time,
            'total_tests': len(stability_results)
        }
        
        print(f"âœ… ç¨³å®šæ€§æµ‹è¯•: æˆåŠŸç‡{success_rate*100:.1f}%, å¹³å‡æ‰§è¡Œæ—¶é—´{avg_time:.3f}s")
        
    async def _test_scalability(self):
        """æµ‹è¯•å¯æ‰©å±•æ€§"""
        print("ğŸ“ˆ æµ‹è¯•å¯æ‰©å±•æ€§...")
        
        scalability_results = {}
        
        for population_size in [5, 10, 15, 20]:
            start_time = time.time()
            population = create_initial_population(population_size)
            creation_time = time.time() - start_time
            
            # è¯„ä¼°æ—¶é—´
            evaluator = SymbolicEvaluator()
            eval_start = time.time()
            for individual in population:
                await evaluator.evaluate(individual)
            eval_time = time.time() - eval_start
            
            scalability_results[population_size] = {
                'creation_time': creation_time,
                'evaluation_time': eval_time,
                'total_time': creation_time + eval_time,
                'time_per_individual': (creation_time + eval_time) / population_size
            }
            
        self.report_data['performance_metrics']['scalability'] = scalability_results
        
        print(f"âœ… å¯æ‰©å±•æ€§æµ‹è¯•: æµ‹è¯•äº†{len(scalability_results)}ç§ç§ç¾¤å¤§å°")
        
    def generate_recommendations(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print("ğŸ’¡ ç”Ÿæˆæ”¹è¿›å»ºè®®...")
        
        recommendations = []
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡çš„å»ºè®®
        symbolic_perf = self.report_data['evaluation_results']['symbolic']
        realworld_perf = self.report_data['evaluation_results']['realworld']
        
        if symbolic_perf['average_score'] < 0.8:
            recommendations.append("å»ºè®®ä¼˜åŒ–ç¬¦å·è¯„ä¼°å™¨ä»¥æé«˜åŸºç¡€æ¨ç†èƒ½åŠ›")
            
        if realworld_perf['average_score'] < 0.7:
            recommendations.append("å»ºè®®å¢å¼ºçœŸå®ä¸–ç•Œè¯„ä¼°çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§")
            
        # åŸºäºå¤šæ ·æ€§çš„å»ºè®®
        diversity_metrics = self.report_data['diversity_metrics']
        if diversity_metrics['diversity_maintenance_rate'] < 0.8:
            recommendations.append("å»ºè®®æ”¹è¿›å¤šæ ·æ€§ç»´æŠ¤æœºåˆ¶ä»¥é˜²æ­¢è¿‡æ—©æ”¶æ•›")
            
        # åŸºäºç¨³å®šæ€§çš„å»ºè®®
        stability = self.report_data['performance_metrics']['stability']
        if stability['success_rate'] < 1.0:
            recommendations.append("å»ºè®®åŠ å¼ºé”™è¯¯å¤„ç†å’Œå¼‚å¸¸æ¢å¤æœºåˆ¶")
            
        # åŸºäºå¯æ‰©å±•æ€§çš„å»ºè®®
        scalability = self.report_data['performance_metrics']['scalability']
        if len(scalability) > 0:
            largest_pop = max(scalability.keys())
            if scalability[largest_pop]['time_per_individual'] > 0.1:
                recommendations.append("å»ºè®®ä¼˜åŒ–å¤§è§„æ¨¡ç§ç¾¤çš„å¤„ç†æ•ˆç‡")
                
        self.report_data['recommendations'] = recommendations
        
        print(f"âœ… ç”Ÿæˆäº†{len(recommendations)}æ¡æ”¹è¿›å»ºè®®")
        
    def save_report(self, filename='model_evaluation_report.json'):
        """ä¿å­˜æŠ¥å‘Š"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        
    def print_summary(self):
        """æ‰“å°æŠ¥å‘Šæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“‹ æ¨¡å‹è¯„ä¼°æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
        print("="*60)
        
        # æµ‹è¯•ä¿¡æ¯
        test_info = self.report_data['test_info']
        print(f"ğŸ“… æµ‹è¯•æ—¥æœŸ: {test_info['test_date']}")
        print(f"â±ï¸  æ€»æµ‹è¯•æ—¶é—´: {test_info['total_test_time']:.2f}ç§’")
        
        # æ€§èƒ½æŒ‡æ ‡
        perf_metrics = self.report_data['performance_metrics']
        if 'population_creation' in perf_metrics:
            creation = perf_metrics['population_creation']
            print(f"ğŸ“Š ç§ç¾¤åˆ›å»º: {creation['individuals_per_second']:.1f}ä¸ªä½“/ç§’")
            
        if 'stability' in perf_metrics:
            stability = perf_metrics['stability']
            print(f"ğŸ”’ ç¨³å®šæ€§: {stability['success_rate']*100:.1f}%æˆåŠŸç‡")
            
        # è¯„ä¼°ç»“æœ
        eval_results = self.report_data['evaluation_results']
        if 'symbolic' in eval_results:
            symbolic = eval_results['symbolic']
            print(f"ğŸ§® ç¬¦å·è¯„ä¼°: å¹³å‡å¾—åˆ†{symbolic['average_score']:.3f} Â± {symbolic['std_score']:.3f}")
            
        if 'realworld' in eval_results:
            realworld = eval_results['realworld']
            print(f"ğŸŒ çœŸå®ä¸–ç•Œè¯„ä¼°: å¹³å‡å¾—åˆ†{realworld['average_score']:.3f} Â± {realworld['std_score']:.3f}")
            
        # è¿›åŒ–åˆ†æ
        evolution = self.report_data['evolution_analysis']
        if evolution:
            print(f"ğŸ”„ è¿›åŒ–æ€§èƒ½: {evolution['generations_per_second']:.1f}ä»£/ç§’")
            print(f"ğŸŒŠ å¤šæ ·æ€§: {evolution['average_diversity']:.3f} Â± {evolution['std_diversity']:.3f}")
            
        # å»ºè®®
        recommendations = self.report_data['recommendations']
        if recommendations:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® ({len(recommendations)}æ¡):")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
                
        print("="*60)

async def main():
    """ä¸»å‡½æ•°"""
    evaluator = ModelEvaluationReport()
    
    # è¿è¡Œå…¨é¢è¯„ä¼°
    await evaluator.run_comprehensive_evaluation()
    
    # ç”Ÿæˆå»ºè®®
    evaluator.generate_recommendations()
    
    # ä¿å­˜æŠ¥å‘Š
    evaluator.save_report()
    
    # æ‰“å°æ‘˜è¦
    evaluator.print_summary()

if __name__ == "__main__":
    asyncio.run(main()) 