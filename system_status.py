#!/usr/bin/env python3
"""
ç³»ç»ŸçŠ¶æ€æ£€æŸ¥è„šæœ¬
ç”¨äºæ£€æŸ¥ç³»ç»Ÿè¿è¡ŒçŠ¶æ€å’Œå¥åº·çŠ¶å†µ
"""

import asyncio
import time
import psutil
import torch
import platform
import sys
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
from config.logging_setup import setup_logging

logger = setup_logging()

@dataclass
class SystemStatus:
    """ç³»ç»ŸçŠ¶æ€æ•°æ®ç±»"""
    timestamp: float
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    memory_available_mb: float
    disk_usage_percent: float
    gpu_memory_percent: float
    gpu_memory_used_mb: float
    network_connections: int
    python_version: str
    torch_version: str
    cuda_available: bool
    cuda_version: str

class SystemStatusChecker:
    """ç³»ç»ŸçŠ¶æ€æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.logger = logger
        self.status_history: List[SystemStatus] = []
        
    async def check_system_resources(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        try:
            self.logger.info("æ£€æŸ¥ç³»ç»Ÿèµ„æº...")
            
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            memory_used_mb = memory.used / (1024 * 1024)
            memory_available_mb = memory.available / (1024 * 1024)
            
            # ç£ç›˜ä½¿ç”¨æƒ…å†µ
            disk = psutil.disk_usage('/')
            disk_usage_percent = (disk.used / disk.total) * 100
            
            # GPUä½¿ç”¨æƒ…å†µ
            gpu_memory_percent = 0
            gpu_memory_used_mb = 0
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated()
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory
                gpu_memory_percent = (gpu_memory / gpu_memory_total) * 100
                gpu_memory_used_mb = gpu_memory / (1024 * 1024)
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                # MPSè®¾å¤‡ä¸æ”¯æŒå†…å­˜ç»Ÿè®¡ï¼Œè®¾ç½®ä¸º0
                gpu_memory_percent = 0
                gpu_memory_used_mb = 0
            
            # ç½‘ç»œè¿æ¥æ•°
            try:
                network_connections = len(psutil.net_connections())
            except psutil.AccessDenied:
                network_connections = 0
            
            # ç³»ç»Ÿä¿¡æ¯
            python_version = sys.version
            torch_version = torch.__version__
            cuda_available = torch.cuda.is_available()
            mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
            cuda_version = torch.version.cuda if cuda_available else "N/A"
            
            # åˆ›å»ºçŠ¶æ€è®°å½•
            status = SystemStatus(
                timestamp=time.time(),
                cpu_percent=cpu_percent,
                memory_percent=memory_percent,
                memory_used_mb=memory_used_mb,
                memory_available_mb=memory_available_mb,
                disk_usage_percent=disk_usage_percent,
                gpu_memory_percent=gpu_memory_percent,
                gpu_memory_used_mb=gpu_memory_used_mb,
                network_connections=network_connections,
                python_version=python_version,
                torch_version=torch_version,
                cuda_available=cuda_available,
                cuda_version=cuda_version
            )
            
            self.status_history.append(status)
            
            # è¯„ä¼°ç³»ç»Ÿå¥åº·çŠ¶æ€
            health_status = self._evaluate_health_status(status)
            
            return {
                "status": "success",
                "data": status,
                "health": health_status
            }
            
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": str(e)
            }
    
    def _evaluate_health_status(self, status: SystemStatus) -> Dict[str, Any]:
        """è¯„ä¼°ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        health_issues = []
        warnings = []
        
        # CPUæ£€æŸ¥
        if status.cpu_percent > 90:
            health_issues.append("CPUä½¿ç”¨ç‡è¿‡é«˜ (>90%)")
        elif status.cpu_percent > 80:
            warnings.append("CPUä½¿ç”¨ç‡è¾ƒé«˜ (>80%)")
            
        # å†…å­˜æ£€æŸ¥
        if status.memory_percent > 95:
            health_issues.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ (>95%)")
        elif status.memory_percent > 85:
            warnings.append("å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ (>85%)")
            
        # ç£ç›˜æ£€æŸ¥
        if status.disk_usage_percent > 95:
            health_issues.append("ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜ (>95%)")
        elif status.disk_usage_percent > 85:
            warnings.append("ç£ç›˜ä½¿ç”¨ç‡è¾ƒé«˜ (>85%)")
            
        # GPUæ£€æŸ¥
        if status.cuda_available and status.gpu_memory_percent > 95:
            health_issues.append("GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ (>95%)")
        elif status.cuda_available and status.gpu_memory_percent > 85:
            warnings.append("GPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ (>85%)")
            
        # ç¡®å®šæ•´ä½“å¥åº·çŠ¶æ€
        if health_issues:
            overall_status = "critical"
        elif warnings:
            overall_status = "warning"
        else:
            overall_status = "healthy"
            
        return {
            "overall_status": overall_status,
            "health_issues": health_issues,
            "warnings": warnings,
            "recommendations": self._generate_recommendations(status, health_issues, warnings)
        }
    
    def _generate_recommendations(self, status: SystemStatus, 
                                health_issues: List[str], 
                                warnings: List[str]) -> List[str]:
        """ç”Ÿæˆç³»ç»Ÿå»ºè®®"""
        recommendations = []
        
        if status.cpu_percent > 80:
            recommendations.append("è€ƒè™‘å‡å°‘å¹¶è¡Œä»»åŠ¡æ•°é‡æˆ–ä¼˜åŒ–ç®—æ³•æ•ˆç‡")
            
        if status.memory_percent > 85:
            recommendations.append("è€ƒè™‘æ¸…ç†å†…å­˜ç¼“å­˜æˆ–å¢åŠ ç³»ç»Ÿå†…å­˜")
            
        if status.disk_usage_percent > 85:
            recommendations.append("è€ƒè™‘æ¸…ç†ç£ç›˜ç©ºé—´æˆ–æ‰©å±•å­˜å‚¨å®¹é‡")
            
        if status.cuda_available and status.gpu_memory_percent > 85:
            recommendations.append("è€ƒè™‘æ¸…ç†GPUç¼“å­˜æˆ–å‡å°‘GPUå†…å­˜ä½¿ç”¨")
            
        if not status.cuda_available and not hasattr(torch.backends, 'mps') or not torch.backends.mps.is_available():
            recommendations.append("è€ƒè™‘å¯ç”¨CUDAæˆ–MPSä»¥æå‡è®¡ç®—æ€§èƒ½")
        elif not status.cuda_available and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            recommendations.append("å·²å¯ç”¨MPSï¼Œå¯è€ƒè™‘å®‰è£…CUDAä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½")
            
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡ŒçŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šæ“ä½œ")
            
        return recommendations
    
    async def check_python_environment(self) -> Dict[str, Any]:
        """æ£€æŸ¥Pythonç¯å¢ƒ"""
        try:
            self.logger.info("æ£€æŸ¥Pythonç¯å¢ƒ...")
            
            # æ£€æŸ¥Pythonç‰ˆæœ¬
            python_version = sys.version_info
            version_ok = python_version.major == 3 and python_version.minor >= 8
            
            # æ£€æŸ¥å¿…è¦çš„åŒ…
            required_packages = {
                'torch': torch.__version__,
                'numpy': 'numpy',  # éœ€è¦å¯¼å…¥æ£€æŸ¥
                'asyncio': 'asyncio',
                'psutil': 'psutil'
            }
            
            missing_packages = []
            package_versions = {}
            
            for package, version in required_packages.items():
                try:
                    if package == 'torch':
                        package_versions[package] = version
                    else:
                        module = __import__(package)
                        package_versions[package] = getattr(module, '__version__', 'unknown')
                except ImportError:
                    missing_packages.append(package)
            
            # æ£€æŸ¥GPUå¯ç”¨æ€§
            cuda_available = torch.cuda.is_available()
            mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
            cuda_version = torch.version.cuda if cuda_available else None
            
            return {
                "status": "success",
                "python_version": {
                    "version": f"{python_version.major}.{python_version.minor}.{python_version.micro}",
                    "ok": version_ok
                },
                "packages": {
                    "available": package_versions,
                    "missing": missing_packages
                },
                "gpu": {
                    "cuda_available": cuda_available,
                    "mps_available": mps_available,
                    "cuda_version": cuda_version
                },
                "environment_ok": version_ok and len(missing_packages) == 0
            }
            
        except Exception as e:
            self.logger.error(f"Pythonç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": str(e)
            }
    
    async def check_core_components(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ ¸å¿ƒç»„ä»¶"""
        try:
            self.logger.info("æ£€æŸ¥æ ¸å¿ƒç»„ä»¶...")
            
            components = {}
            
            # æ£€æŸ¥æ¨¡å‹æ¨¡å—
            try:
                from models.modular_net import ModularMathReasoningNet
                components['models'] = {"status": "available", "message": "æ¨¡å‹æ¨¡å—æ­£å¸¸"}
            except ImportError as e:
                components['models'] = {"status": "error", "message": f"æ¨¡å‹æ¨¡å—å¯¼å…¥å¤±è´¥: {e}"}
            
            # æ£€æŸ¥è¿›åŒ–æ¨¡å—
            try:
                from evolution.population import create_initial_population
                from evolution.nsga2 import evolve_population_nsga2_simple
                components['evolution'] = {"status": "available", "message": "è¿›åŒ–æ¨¡å—æ­£å¸¸"}
            except ImportError as e:
                components['evolution'] = {"status": "error", "message": f"è¿›åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}"}
            
            # æ£€æŸ¥è¯„ä¼°å™¨æ¨¡å—
            try:
                from evaluators.symbolic_evaluator import SymbolicEvaluator
                from evaluators.realworld_evaluator import RealWorldEvaluator
                components['evaluators'] = {"status": "available", "message": "è¯„ä¼°å™¨æ¨¡å—æ­£å¸¸"}
            except ImportError as e:
                components['evaluators'] = {"status": "error", "message": f"è¯„ä¼°å™¨æ¨¡å—å¯¼å…¥å¤±è´¥: {e}"}
            
            # æ£€æŸ¥æ•°æ®æ¨¡å—
            try:
                from data.generator import RealWorldDataGenerator
                components['data'] = {"status": "available", "message": "æ•°æ®æ¨¡å—æ­£å¸¸"}
            except ImportError as e:
                components['data'] = {"status": "error", "message": f"æ•°æ®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}"}
            
            # æ£€æŸ¥é…ç½®æ¨¡å—
            try:
                from config.logging_setup import setup_logging
                components['config'] = {"status": "available", "message": "é…ç½®æ¨¡å—æ­£å¸¸"}
            except ImportError as e:
                components['config'] = {"status": "error", "message": f"é…ç½®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}"}
            
            # ç»Ÿè®¡ç»„ä»¶çŠ¶æ€
            total_components = len(components)
            available_components = sum(1 for c in components.values() if c["status"] == "available")
            error_components = total_components - available_components
            
            return {
                "status": "success",
                "components": components,
                "summary": {
                    "total": total_components,
                    "available": available_components,
                    "errors": error_components,
                    "all_ok": error_components == 0
                }
            }
            
        except Exception as e:
            self.logger.error(f"æ ¸å¿ƒç»„ä»¶æ£€æŸ¥å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": str(e)
            }
    
    async def check_performance_benchmarks(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ€§èƒ½åŸºå‡†"""
        try:
            self.logger.info("æ£€æŸ¥æ€§èƒ½åŸºå‡†...")
            
            benchmarks = {}
            
            # æ¨¡å‹åˆ›å»ºæ€§èƒ½
            try:
                from models.modular_net import ModularMathReasoningNet
                start_time = time.time()
                model = ModularMathReasoningNet(modules_config=[])
                creation_time = time.time() - start_time
                benchmarks['model_creation'] = {
                    "time": creation_time,
                    "status": "fast" if creation_time < 0.1 else "normal" if creation_time < 0.5 else "slow"
                }
            except Exception as e:
                benchmarks['model_creation'] = {"error": str(e)}
            
            # ç§ç¾¤åˆ›å»ºæ€§èƒ½
            try:
                from evolution.population import create_initial_population
                start_time = time.time()
                population = create_initial_population(10)
                creation_time = time.time() - start_time
                benchmarks['population_creation'] = {
                    "time": creation_time,
                    "status": "fast" if creation_time < 0.5 else "normal" if creation_time < 2.0 else "slow"
                }
            except Exception as e:
                benchmarks['population_creation'] = {"error": str(e)}
            
            # è¯„ä¼°æ€§èƒ½
            try:
                from evaluators.symbolic_evaluator import SymbolicEvaluator
                from models.modular_net import ModularMathReasoningNet
                
                model = ModularMathReasoningNet(modules_config=[])
                evaluator = SymbolicEvaluator()
                
                start_time = time.time()
                score = await evaluator.evaluate(model, level=0)
                evaluation_time = time.time() - start_time
                
                benchmarks['evaluation'] = {
                    "time": evaluation_time,
                    "score": score,
                    "status": "fast" if evaluation_time < 1.0 else "normal" if evaluation_time < 5.0 else "slow"
                }
            except Exception as e:
                benchmarks['evaluation'] = {"error": str(e)}
            
            # è¿›åŒ–æ€§èƒ½
            try:
                from evolution.nsga2 import evolve_population_nsga2_simple
                from evolution.population import create_initial_population
                
                population = create_initial_population(5)
                fitness_scores = [(0.8, 0.7)] * len(population)
                
                start_time = time.time()
                evolved_population = evolve_population_nsga2_simple(
                    population, fitness_scores, 0.8, 0.8
                )
                evolution_time = time.time() - start_time
                
                benchmarks['evolution'] = {
                    "time": evolution_time,
                    "status": "fast" if evolution_time < 0.5 else "normal" if evolution_time < 2.0 else "slow"
                }
            except Exception as e:
                benchmarks['evolution'] = {"error": str(e)}
            
            return {
                "status": "success",
                "benchmarks": benchmarks
            }
            
        except Exception as e:
            self.logger.error(f"æ€§èƒ½åŸºå‡†æ£€æŸ¥å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": str(e)
            }
    
    async def generate_status_report(self) -> str:
        """ç”ŸæˆçŠ¶æ€æŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆç³»ç»ŸçŠ¶æ€æŠ¥å‘Š...")
        
        # æ”¶é›†æ‰€æœ‰æ£€æŸ¥ç»“æœ
        resources_result = await self.check_system_resources()
        environment_result = await self.check_python_environment()
        components_result = await self.check_core_components()
        benchmarks_result = await self.check_performance_benchmarks()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
ğŸ“Š ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š
================

â° ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ–¥ï¸  ç³»ç»Ÿèµ„æºçŠ¶æ€:
"""
        
        if resources_result["status"] == "success":
            status = resources_result["data"]
            health = resources_result["health"]
            
            report += f"""
   CPUä½¿ç”¨ç‡: {status.cpu_percent:.1f}%
   å†…å­˜ä½¿ç”¨ç‡: {status.memory_percent:.1f}% ({status.memory_used_mb:.1f} MB)
   å¯ç”¨å†…å­˜: {status.memory_available_mb:.1f} MB
   ç£ç›˜ä½¿ç”¨ç‡: {status.disk_usage_percent:.1f}%
   GPUå†…å­˜ä½¿ç”¨ç‡: {status.gpu_memory_percent:.1f}% ({status.gpu_memory_used_mb:.1f} MB)
   ç½‘ç»œè¿æ¥æ•°: {status.network_connections}
   
   å¥åº·çŠ¶æ€: {health['overall_status'].upper()}
"""
            
            if health['health_issues']:
                report += "   âš ï¸  å¥åº·é—®é¢˜:\n"
                for issue in health['health_issues']:
                    report += f"      - {issue}\n"
                    
            if health['warnings']:
                report += "   âš ï¸  è­¦å‘Š:\n"
                for warning in health['warnings']:
                    report += f"      - {warning}\n"
                    
            if health['recommendations']:
                report += "   ğŸ’¡ å»ºè®®:\n"
                for rec in health['recommendations']:
                    report += f"      - {rec}\n"
        else:
            report += f"   âŒ æ£€æŸ¥å¤±è´¥: {resources_result['message']}\n"
        
        report += f"""
ğŸ Pythonç¯å¢ƒçŠ¶æ€:
"""
        
        if environment_result["status"] == "success":
            env = environment_result
            report += f"""
   Pythonç‰ˆæœ¬: {env['python_version']['version']} {'âœ…' if env['python_version']['ok'] else 'âŒ'}
   CUDAå¯ç”¨: {'âœ…' if env['gpu']['cuda_available'] else 'âŒ'}
   MPSå¯ç”¨: {'âœ…' if env['gpu']['mps_available'] else 'âŒ'}
   CUDAç‰ˆæœ¬: {env['gpu']['cuda_version'] or 'N/A'}
   
   å·²å®‰è£…åŒ…:
"""
            for package, version in env['packages']['available'].items():
                report += f"      - {package}: {version}\n"
                
            if env['packages']['missing']:
                report += "   ç¼ºå¤±åŒ…:\n"
                for package in env['packages']['missing']:
                    report += f"      - {package}\n"
        else:
            report += f"   âŒ æ£€æŸ¥å¤±è´¥: {environment_result['message']}\n"
        
        report += f"""
ğŸ”§ æ ¸å¿ƒç»„ä»¶çŠ¶æ€:
"""
        
        if components_result["status"] == "success":
            comps = components_result["components"]
            summary = components_result["summary"]
            
            for name, comp in comps.items():
                status_icon = "âœ…" if comp["status"] == "available" else "âŒ"
                report += f"   {status_icon} {name}: {comp['message']}\n"
                
            report += f"\n   ç»„ä»¶ç»Ÿè®¡: {summary['available']}/{summary['total']} æ­£å¸¸\n"
        else:
            report += f"   âŒ æ£€æŸ¥å¤±è´¥: {components_result['message']}\n"
        
        report += f"""
âš¡ æ€§èƒ½åŸºå‡†:
"""
        
        if benchmarks_result["status"] == "success":
            benchs = benchmarks_result["benchmarks"]
            
            for name, bench in benchs.items():
                if "error" in bench:
                    report += f"   âŒ {name}: {bench['error']}\n"
                else:
                    status_icon = "ğŸŸ¢" if bench["status"] == "fast" else "ğŸŸ¡" if bench["status"] == "normal" else "ğŸ”´"
                    report += f"   {status_icon} {name}: {bench['time']:.3f}ç§’ ({bench['status']})\n"
        else:
            report += f"   âŒ æ£€æŸ¥å¤±è´¥: {benchmarks_result['message']}\n"
        
        report += f"""
ğŸ“ˆ ç³»ç»Ÿè¯„çº§:
"""
        
        # æ”¹è¿›çš„ç»¼åˆè¯„çº§ç®—æ³•
        overall_score = 0
        max_possible_score = 0
        
        # 1. ç³»ç»Ÿèµ„æºè¯„åˆ† (30åˆ†)
        if resources_result["status"] == "success":
            health = resources_result["health"]
            status = resources_result["data"]
            
            # åŸºç¡€åˆ†ï¼šå¥åº·çŠ¶æ€
            if health["overall_status"] == "healthy":
                overall_score += 20
            elif health["overall_status"] == "warning":
                overall_score += 15
            elif health["overall_status"] == "critical":
                overall_score += 5
            
            # æ€§èƒ½åŠ åˆ†ï¼šå†…å­˜ä½¿ç”¨ç‡
            if status.memory_percent < 50:
                overall_score += 10
            elif status.memory_percent < 70:
                overall_score += 5
            elif status.memory_percent < 85:
                overall_score += 2
            
            # GPUåŠ åˆ†
            if status.gpu_memory_percent > 0 or hasattr(status, 'mps_available') and status.mps_available:
                overall_score += 5
            
            max_possible_score += 30
        
        # 2. Pythonç¯å¢ƒè¯„åˆ† (25åˆ†)
        if environment_result["status"] == "success":
            env = environment_result
            
            # åŸºç¡€åˆ†ï¼šç¯å¢ƒæ­£å¸¸
            if env["environment_ok"]:
                overall_score += 15
            
            # Pythonç‰ˆæœ¬åŠ åˆ†
            if env["python_version"]["ok"]:
                overall_score += 5
            
            # GPUæ”¯æŒåŠ åˆ†
            if env["gpu"]["cuda_available"] or env["gpu"]["mps_available"]:
                overall_score += 5
            
            max_possible_score += 25
        
        # 3. æ ¸å¿ƒç»„ä»¶è¯„åˆ† (25åˆ†)
        if components_result["status"] == "success":
            summary = components_result["summary"]
            
            # åŸºç¡€åˆ†ï¼šæ‰€æœ‰ç»„ä»¶æ­£å¸¸
            if summary["all_ok"]:
                overall_score += 20
            
            # å¯ç”¨ç»„ä»¶æ¯”ä¾‹åŠ åˆ†
            component_ratio = summary["available"] / summary["total"]
            overall_score += int(component_ratio * 5)
            
            max_possible_score += 25
        
        # 4. æ€§èƒ½åŸºå‡†è¯„åˆ† (20åˆ†)
        if benchmarks_result["status"] == "success":
            benchs = benchmarks_result["benchmarks"]
            
            # è®¡ç®—å¿«é€ŸåŸºå‡†æ•°é‡
            fast_benchmarks = sum(1 for b in benchs.values() 
                                if "status" in b and b["status"] == "fast")
            normal_benchmarks = sum(1 for b in benchs.values() 
                                  if "status" in b and b["status"] == "normal")
            
            # åŸºå‡†æ€§èƒ½è¯„åˆ†
            if fast_benchmarks >= 3:
                overall_score += 15
            elif fast_benchmarks >= 2:
                overall_score += 10
            elif fast_benchmarks >= 1:
                overall_score += 5
            
            # ç¨³å®šæ€§åŠ åˆ†
            if normal_benchmarks + fast_benchmarks >= len(benchs) * 0.8:
                overall_score += 5
            
            max_possible_score += 20
        
        # è®¡ç®—æœ€ç»ˆè¯„åˆ†
        if max_possible_score > 0:
            final_score = (overall_score / max_possible_score) * 100
            
            # æ”¹è¿›çš„è¯„çº§æ ‡å‡†
            if final_score >= 85:
                grade = "ğŸŸ¢ ä¼˜ç§€"
            elif final_score >= 70:
                grade = "ğŸŸ¡ è‰¯å¥½"
            elif final_score >= 50:
                grade = "ğŸŸ  ä¸€èˆ¬"
            elif final_score >= 30:
                grade = "ğŸŸ¡ éœ€è¦æ”¹è¿›"
            else:
                grade = "ğŸ”´ éœ€è¦ä¼˜åŒ–"
                
            report += f"   ç»¼åˆè¯„åˆ†: {final_score:.1f}/100 {grade}\n"
        else:
            report += f"   ç»¼åˆè¯„åˆ†: æ— æ³•è®¡ç®— (æ£€æŸ¥å¤±è´¥)\n"
        
        report += "\nğŸ‰ ç³»ç»ŸçŠ¶æ€æ£€æŸ¥å®Œæˆï¼"
        
        return report

async def main():
    """ä¸»å‡½æ•°"""
    checker = SystemStatusChecker()
    report = await checker.generate_status_report()
    print(report)

if __name__ == "__main__":
    asyncio.run(main()) 