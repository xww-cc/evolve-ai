#!/usr/bin/env python3
"""
è¯„ä¼°è¯Šæ–­å·¥å…·
ç”¨äºè¯Šæ–­å’Œä¿®å¤è¯„ä¼°å¤±è´¥çš„é—®é¢˜
"""

import torch
import logging
import asyncio
from typing import Dict, List, Optional
from models.modular_net import ModularMathReasoningNet
from evaluators.symbolic_evaluator import SymbolicEvaluator
from evaluators.realworld_evaluator import RealWorldEvaluator
from evaluators.vision_evaluator import VisionEvaluator
from config.logging_setup import setup_logging

logger = setup_logging()

class EvaluationDiagnostic:
    """è¯„ä¼°è¯Šæ–­å·¥å…·"""
    
    def __init__(self):
        self.symbolic_evaluator = SymbolicEvaluator()
        self.realworld_evaluator = RealWorldEvaluator()
        self.vision_evaluator = VisionEvaluator(hidden_dim=256)
        
    async def diagnose_evaluation_issues(self) -> Dict:
        """è¯Šæ–­è¯„ä¼°é—®é¢˜"""
        print("ğŸ” å¼€å§‹è¯„ä¼°è¯Šæ–­...")
        
        issues = {
            'model_creation': self._test_model_creation(),
            'basic_forward': self._test_basic_forward(),
            'symbolic_evaluation': await self._test_symbolic_evaluation(),
            'realworld_evaluation': await self._test_realworld_evaluation(),
            'vision_evaluation': self._test_vision_evaluation(),
            'memory_usage': self._test_memory_usage(),
            'gpu_availability': self._test_gpu_availability()
        }
        
        return issues
    
    def _test_model_creation(self) -> Dict:
        """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
        try:
            # åˆ›å»ºåŸºç¡€æ¨¡å‹ - ä½¿ç”¨æ­£ç¡®çš„é…ç½®æ ¼å¼
            modules_config = [
                {
                    'input_dim': 4,
                    'output_dim': 64,
                    'widths': [32, 64],
                    'activation_fn_name': 'relu',
                    'use_batchnorm': True,
                    'module_type': 'mlp',
                    'input_source': 'initial_input'
                },
                {
                    'input_dim': 64,
                    'output_dim': 32,
                    'widths': [48, 32],
                    'activation_fn_name': 'relu',
                    'use_batchnorm': True,
                    'module_type': 'mlp',
                    'input_source': 'module_0'
                }
            ]
            
            model = ModularMathReasoningNet(
                modules_config=modules_config,
                epigenetic_markers={'learning_rate': 0.001}
            )
            
            return {
                'status': 'success',
                'message': 'æ¨¡å‹åˆ›å»ºæˆåŠŸ',
                'details': {
                    'total_params': sum(p.numel() for p in model.parameters()),
                    'model_type': type(model).__name__,
                    'num_modules': len(model.subnet_modules)
                }
            }
        except Exception as e:
            return {
                'status': 'error',
                'message': f'æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}',
                'details': {'error': str(e)}
            }
    
    def _test_basic_forward(self) -> Dict:
        """æµ‹è¯•åŸºç¡€å‰å‘ä¼ æ’­"""
        try:
            # ä½¿ç”¨æ­£ç¡®çš„é…ç½®æ ¼å¼
            modules_config = [
                {
                    'input_dim': 4,
                    'output_dim': 64,
                    'widths': [32, 64],
                    'activation_fn_name': 'relu',
                    'use_batchnorm': True,
                    'module_type': 'mlp',
                    'input_source': 'initial_input'
                },
                {
                    'input_dim': 64,
                    'output_dim': 32,
                    'widths': [48, 32],
                    'activation_fn_name': 'relu',
                    'use_batchnorm': True,
                    'module_type': 'mlp',
                    'input_source': 'module_0'
                }
            ]
            
            model = ModularMathReasoningNet(
                modules_config=modules_config,
                epigenetic_markers={'learning_rate': 0.001}
            )
            
            # æµ‹è¯•è¾“å…¥
            test_input = torch.randn(2, 4)
            output = model(test_input)
            
            return {
                'status': 'success',
                'message': 'åŸºç¡€å‰å‘ä¼ æ’­æˆåŠŸ',
                'details': {
                    'input_shape': test_input.shape,
                    'output_shape': output.shape,
                    'output_stats': {
                        'mean': output.mean().item(),
                        'std': output.std().item(),
                        'min': output.min().item(),
                        'max': output.max().item()
                    }
                }
            }
        except Exception as e:
            return {
                'status': 'error',
                'message': f'åŸºç¡€å‰å‘ä¼ æ’­å¤±è´¥: {e}',
                'details': {'error': str(e)}
            }
    
    async def _test_symbolic_evaluation(self) -> Dict:
        """æµ‹è¯•ç¬¦å·è¯„ä¼°"""
        try:
            # ä½¿ç”¨æ­£ç¡®çš„é…ç½®æ ¼å¼
            modules_config = [
                {
                    'input_dim': 4,
                    'output_dim': 64,
                    'widths': [32, 64],
                    'activation_fn_name': 'relu',
                    'use_batchnorm': True,
                    'module_type': 'mlp',
                    'input_source': 'initial_input'
                },
                {
                    'input_dim': 64,
                    'output_dim': 32,
                    'widths': [48, 32],
                    'activation_fn_name': 'relu',
                    'use_batchnorm': True,
                    'module_type': 'mlp',
                    'input_source': 'module_0'
                }
            ]
            
            model = ModularMathReasoningNet(
                modules_config=modules_config,
                epigenetic_markers={'learning_rate': 0.001}
            )
            
            # æµ‹è¯•ç¬¦å·è¯„ä¼° - ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬
            score = await self.symbolic_evaluator.evaluate(model, level=0)
            
            return {
                'status': 'success',
                'message': 'ç¬¦å·è¯„ä¼°æˆåŠŸ',
                'details': {
                    'score': score,
                    'evaluator_type': type(self.symbolic_evaluator).__name__
                }
            }
        except Exception as e:
            return {
                'status': 'error',
                'message': f'ç¬¦å·è¯„ä¼°å¤±è´¥: {e}',
                'details': {'error': str(e)}
            }
    
    async def _test_realworld_evaluation(self) -> Dict:
        """æµ‹è¯•çœŸå®ä¸–ç•Œè¯„ä¼°"""
        try:
            # ä½¿ç”¨æ­£ç¡®çš„é…ç½®æ ¼å¼
            modules_config = [
                {
                    'input_dim': 4,
                    'output_dim': 64,
                    'widths': [32, 64],
                    'activation_fn_name': 'relu',
                    'use_batchnorm': True,
                    'module_type': 'mlp',
                    'input_source': 'initial_input'
                },
                {
                    'input_dim': 64,
                    'output_dim': 32,
                    'widths': [48, 32],
                    'activation_fn_name': 'relu',
                    'use_batchnorm': True,
                    'module_type': 'mlp',
                    'input_source': 'module_0'
                }
            ]
            
            model = ModularMathReasoningNet(
                modules_config=modules_config,
                epigenetic_markers={'learning_rate': 0.001}
            )
            
            # æµ‹è¯•çœŸå®ä¸–ç•Œè¯„ä¼° - ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬
            score = await self.realworld_evaluator.evaluate(model)
            
            return {
                'status': 'success',
                'message': 'çœŸå®ä¸–ç•Œè¯„ä¼°æˆåŠŸ',
                'details': {
                    'score': score,
                    'evaluator_type': type(self.realworld_evaluator).__name__
                }
            }
        except Exception as e:
            return {
                'status': 'error',
                'message': f'çœŸå®ä¸–ç•Œè¯„ä¼°å¤±è´¥: {e}',
                'details': {'error': str(e)}
            }
    
    def _test_vision_evaluation(self) -> Dict:
        """æµ‹è¯•è§†è§‰è¯„ä¼°"""
        try:
            # åˆ›å»ºæµ‹è¯•è§†è§‰ç‰¹å¾
            test_features = torch.randn(2, 10, 256)
            
            # æµ‹è¯•è§†è§‰è¯„ä¼°
            evaluation_results = self.vision_evaluator(test_features)
            
            return {
                'status': 'success',
                'message': 'è§†è§‰è¯„ä¼°æˆåŠŸ',
                'details': {
                    'overall_score': evaluation_results['overall_score'].mean().item(),
                    'understanding_score': evaluation_results['understanding_score'].mean().item(),
                    'reasoning_score': evaluation_results['reasoning_score'].mean().item(),
                    'creation_score': evaluation_results['creation_score'].mean().item(),
                    'spatial_score': evaluation_results['spatial_score'].mean().item(),
                    'comprehensive_score': evaluation_results['comprehensive_score'].mean().item()
                }
            }
        except Exception as e:
            return {
                'status': 'error',
                'message': f'è§†è§‰è¯„ä¼°å¤±è´¥: {e}',
                'details': {'error': str(e)}
            }
    
    def _test_memory_usage(self) -> Dict:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            
            return {
                'status': 'success',
                'message': f'å†…å­˜ä½¿ç”¨æ­£å¸¸ ({memory.percent:.1f}%)',
                'details': {
                    'total_memory': memory.total,
                    'available_memory': memory.available,
                    'memory_percent': memory.percent
                }
            }
        except Exception as e:
            return {
                'status': 'error',
                'message': f'å†…å­˜æ£€æŸ¥å¤±è´¥: {e}',
                'details': {'error': str(e)}
            }
    
    def _test_gpu_availability(self) -> Dict:
        """æµ‹è¯•GPUå¯ç”¨æ€§"""
        try:
            cuda_available = torch.cuda.is_available()
            
            if cuda_available:
                gpu_count = torch.cuda.device_count()
                current_device = torch.cuda.current_device()
                device_name = torch.cuda.get_device_name(current_device)
                
                return {
                    'status': 'success',
                    'message': f'GPUå¯ç”¨: {device_name}',
                    'details': {
                        'gpu_count': gpu_count,
                        'current_device': current_device,
                        'device_name': device_name
                    }
                }
            else:
                return {
                    'status': 'warning',
                    'message': 'GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU',
                    'details': {'cuda_available': False}
                }
        except Exception as e:
            return {
                'status': 'error',
                'message': f'GPUæ£€æŸ¥å¤±è´¥: {e}',
                'details': {'error': str(e)}
            }
    
    def print_diagnostic_report(self, issues: Dict):
        """æ‰“å°è¯Šæ–­æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ” è¯„ä¼°è¯Šæ–­æŠ¥å‘Š")
        print("="*60)
        
        total_tests = len(issues)
        successful_tests = sum(1 for issue in issues.values() if issue['status'] == 'success')
        warning_tests = sum(1 for issue in issues.values() if issue['status'] == 'warning')
        error_tests = sum(1 for issue in issues.values() if issue['status'] == 'error')
        
        print(f"ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   æˆåŠŸ: {successful_tests}")
        print(f"   è­¦å‘Š: {warning_tests}")
        print(f"   é”™è¯¯: {error_tests}")
        print()
        
        for test_name, result in issues.items():
            status_icon = {
                'success': 'âœ…',
                'warning': 'âš ï¸',
                'error': 'âŒ'
            }.get(result['status'], 'â“')
            
            print(f"{status_icon} {test_name}: {result['message']}")
            
            if result['status'] == 'error' and 'details' in result:
                print(f"   é”™è¯¯è¯¦æƒ…: {result['details'].get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        print("\n" + "="*60)
        
        if error_tests == 0:
            print("ğŸ‰ æ‰€æœ‰è¯„ä¼°æµ‹è¯•é€šè¿‡ï¼")
        else:
            print(f"âš ï¸  å‘ç° {error_tests} ä¸ªè¯„ä¼°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯è¯¦æƒ…ã€‚")
        
        print("="*60)

async def main():
    """ä¸»å‡½æ•°"""
    diagnostic = EvaluationDiagnostic()
    issues = await diagnostic.diagnose_evaluation_issues()
    diagnostic.print_diagnostic_report(issues)

if __name__ == "__main__":
    asyncio.run(main()) 